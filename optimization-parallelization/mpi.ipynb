{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d2f401-34be-4044-a9f5-b8191c30b9eb",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "you need to install\n",
    "```sh\n",
    "mamba install -c conda-forge mpi4py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29eac9-6c0a-4d8e-b547-6befee3efaf9",
   "metadata": {},
   "source": [
    "#  MPI parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f5ba4-2d26-4470-924b-5959eace0ce5",
   "metadata": {},
   "source": [
    "* MPI has been a parallelization method since 1991! Many languages are supported (FORTRAN, C, C++, Python, Julia, Java, R...)\n",
    "* It is used in many large-scale scientific and industrial projects\n",
    "* The way MPI works is that is executes the same file on multiple machines, but each instance gets a RANK (an integer). \n",
    "* The **advantage of MPI** over *multiprocessing* or *concurrent.futures* is that it can run on a large cluster of machines \n",
    "* **The disadvantage** is that memory is not shared, it has to be communicated explicity!\n",
    "\n",
    "* It is up to the code to decide what it should do with that rank: usually it is used to define the part of the problem the worker instance should work on.  There is no \"master\" instance, all workers can send messages to each other (by rank) or wait for a message (blocking or not).  \n",
    "\n",
    "Like other parallel systems, *MPI jobs cannot run in a notebook*: they have to be explicit files.\n",
    "\n",
    "In this example (adapted from https://mpi4py.readthedocs.io/en/stable/tutorial.html), the worker that gets Rank 0 computes some data, and then sends it to the Rank 1 worker, who prints it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b18de7-b9fc-45e7-b14a-d16032f8fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mpi_demo.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(f\"{rank}: Hello!\")\n",
    "\n",
    "if rank == 0:\n",
    "    data = numpy.arange(100, dtype=numpy.float64)\n",
    "    print(f\"{rank}: sending {data}\")\n",
    "    comm.Send(data, dest=1, tag=13)\n",
    "    \n",
    "elif rank == 1:\n",
    "    print(f\"{rank}: waiting for data from rank 0\")\n",
    "    data = numpy.empty(100, dtype=numpy.float64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    print(f\"{rank}: received {data}\")\n",
    "else:\n",
    "    print(f\"{rank}: I've got nothing to do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb4fd6-cfa6-4704-9d4b-743d90c3af58",
   "metadata": {},
   "source": [
    "####  Launch the jobs using a terminal:\n",
    "```sh\n",
    "mpiexec --np 5 python -m mpi4py  mpi_demo.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c0d2e-b37c-45c7-bb44-7ed0aa9cbb34",
   "metadata": {},
   "source": [
    "On a cluster with MPI set up on multiple machines, this would parallelize across 5 machines.  On a single machine, it will run similar to multi-processing in 5 processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e85f47-e4eb-4f6f-ad05-c9327ebf4ae0",
   "metadata": {},
   "source": [
    "### How would you use this?\n",
    "\n",
    "Imagine you have a block-parallel problem.  You can for each worker:\n",
    "\n",
    "1. Set up the problem and initial conditions (identical for all)\n",
    "1. From the rank, decide which block you are (e.g. geometically)\n",
    "1. Compute the first iteration from the initial conditions\n",
    "1. For each *edge* of your block, \n",
    "    1. send its contents to the appropriate neighbor\n",
    "    1. wait to receive the edges from your neighbors\n",
    "1. do the next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf6e21-8b50-467a-80c4-db4ef6e49d44",
   "metadata": {},
   "source": [
    "the MPI standard (and mpi4py) provides a lot of helpful functions for this type of work, e.g. to:\n",
    "* *scatter/gather* data to/from many other workers\n",
    "* *broadcast* data to all workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28bdb8-e22e-46d0-97f5-fec080df3a73",
   "metadata": {},
   "source": [
    "### higher-level: concurrent.futures\n",
    "mpi2py provides a `MPIPoolExecutor` that works exactly like `ProcessPoolExecutor` or `ThreadPoolExecutor`, which makes it easy to apply to map-style problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db26f8a-c873-48e1-9606-6b35793c8893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school2021",
   "language": "python",
   "name": "school2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
