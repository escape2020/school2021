{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b><span style=\"color:blue\">\"HEP Python ecosystem example\"</span></b></h1></center>\n",
    "\n",
    "#### **Goal of this exploration**\n",
    "\n",
    "As said before, other groups and projects are working on other domain-specific areas towards the common goal of a modern, Pythonic and friendly analysis ecosystem for HEP. Let's play a bit with the `phasespace` and `zfit` packages from the [zfit project](https://github.com/zfit/).\n",
    "\n",
    "We will generate some toy simulation data according to a model we want to look at, and will fit the data. This is a common task in HEP analysis. We will even make it a bit more complicated ... and will use `TensorFlow`.\n",
    "\n",
    "Do not hesitate to play around, change things, use the ecosystem to produce plots differently, etc.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;<br><center><img src=\"images/logo_zfit.png\" style=\"width:150px;\"/></center>\n",
    "\n",
    "<center><h2><b><span style=\"color:green\">Model building and fitting library based on TensorFlow</span></b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `zfit` package is a model fitting library based on TensorFlow and optimised for simple and direct manipulation of probability density functions. The main focus is on the scalability, parallelisation and a Pythonic user friendly experience framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A HEP common use-case - simultaneous fit to a signal decay and a control mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Acknowledgements:</b>\n",
    "\n",
    "This tutorial is largely based on code kindly provided by Jonas Eschle (University of Zurich), the main author and maintainer of zfit.\n",
    "It showcases the generation and fit of samples of $ B^0 \\to \\mu^+ \\mu^- K^{*0}$ events (for the sake of example),\n",
    "including a simultaneous fit, and the subsequent determination of the signal significance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbinned maximum likelihood model fit to $B^0 \\to \\mu^+ \\mu^- K^{*0}$\n",
    "\n",
    "The fit will be done in three steps using the `zfit` package:\n",
    "- A fit with the (exponential) combinatorial background to the right-hand side of the signal peak, to obtain good starting values for the fitting parameters.\n",
    "- A model composed of an exponential for the combinatorial background and a double Crystal Ball (DoubleCB) function for the $B^0 \\to \\mu^+ \\mu^- K^{*0}$ signal is built and fitted to the \"rare\" mode. This is hard to get right, as we set low statistics in the rare mode.\n",
    "- To improve the fit, the same model with a few shared parameters is built for the resonant mode $B^0 \\to J/\\psi (\\to \\mu^+ \\mu^-) K^{*0}$,\n",
    "  to improve the shape of the DoubleCB, as mostly the tails are tricky. They seem to be independent of $q^2 (\\mu\\mu)$, so we can share them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Imports, settings, helper functions**\n",
    "\n",
    "Let's start by importing\n",
    "- Standard library modules\n",
    "- Used Scikit-HEP packages\n",
    "- `phasespace` and `zfit`\n",
    "\n",
    "... this is what is meant by working with an ecosystem (where you take what you need, and no more) ;-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from particle.particle import literals as lp\n",
    "from hepstats import hypotests\n",
    "\n",
    "from phasespace import GenParticle\n",
    "\n",
    "import os\n",
    "os.environ['ZFIT_DISABLE_TF_WARNINGS'] = \"True\"  # suppresses TensorFlow warnings\n",
    "import zfit\n",
    "print('zfit version:', zfit.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11  # a \"good\" seed\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Take the PDG particle masses from the Particle package.\n",
    "B0_MASS = lp.B_0.mass\n",
    "JPSI_MASS = lp.Jpsi_1S.mass\n",
    "KSTARZ_MASS = lp.Kst_892_0.mass\n",
    "KSTARZ_WIDTH = lp.Kst_892_0.width\n",
    "PION_MASS = lp.pi_minus.mass\n",
    "KAON_MASS = lp.K_plus.mass\n",
    "MU_MASS = lp.mu_minus.mass\n",
    "\n",
    "# Yields - feel free to change these to play around.\n",
    "n_sig_rare = 120\n",
    "n_sig_reso = 6000\n",
    "n_bkg_rare = 5000\n",
    "n_bkg_reso = 4000\n",
    "\n",
    "# Detector smearing of the particles 4-momenta (sigma of a Gaussian).\n",
    "rare_smearing = 7\n",
    "reso_smearing = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invariant_mass(four_momenta):\n",
    "    \"\"\"Calculate the invariant mass given four momenta with shape (n_events, 4) with px, py, pz, E.\"\"\"\n",
    "    momenta_squared = four_momenta ** 2\n",
    "    return np.sqrt(momenta_squared[:, 3] - np.sum((momenta_squared[:, :3]), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the generation of the decays will be kinematically sampled from a fixed B mass, currently the reconstructed invariant B mass would only\n",
    "be a sharp peak. To include detector resolution effects and make it look more like real data/MC, the particles are smeared according to the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_momenta(four_momenta, smearing=10):\n",
    "    \"\"\"Smears the momenta (gaussian) with a width of `smearing`.\"\"\"\n",
    "    # The four_momenta are \"eager_tensors\", wrapped numpy arrays. \n",
    "    # We could convert them with `np.array(four_momenta)`\n",
    "    return np.random.normal(loc=four_momenta, scale=smearing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Phasespace generation of the rare signal**\n",
    "\n",
    "To generate the signal, we will sample from the phasespace of the decay and apply some ad-hoc smearing.\n",
    "Using the `phasespace` package, a decay chain has to be sticked together with the different particles.\n",
    "While this is simple to do for non-resonant particles with a fixed mass, since it is only a number,\n",
    "phasespace allows also for (arbitrary) resonance shapes of the mass. Therefore, instead of a constant number,\n",
    "a function that samples from the resonance can be provided, whereby `min_mass` and `max_mass` specify the kinematically\n",
    "allowed boundaries for the mass. This is best illustrated with the example below.\n",
    "\n",
    "The $K^{*0}$ is resonant; we define here the sampling of the mass.\n",
    "Either zfit PDFs or TensorFlow Probability distributions (or anything using TensorFlow, also arbitrary\n",
    "Python functions wrapped with `tf.py_function`, can be used). White TensorFlow Probability offers many useful\n",
    "distributions, they often have support from -inf to inf, while zfit PDFs offer the possitility to sample only\n",
    "within certain limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kstar_mass(min_mass, max_mass, n_events):\n",
    "    # Make sure everything is float64.\n",
    "    min_mass = tf.cast(min_mass, tf.float64)\n",
    "    max_mass = tf.cast(max_mass, tf.float64)\n",
    "    kstar_width_cast = tf.cast(KSTARZ_WIDTH, tf.float64)\n",
    "    kstar_mass_cast = tf.cast(KSTARZ_MASS, dtype=tf.float64)\n",
    "\n",
    "    # To produce n_events samples, we can e.g. broadcast the K*0 mass (currently a scalar)\n",
    "    # to have the right shape (n_events,). But there are many ways, also see TensorFlow Probability Distributions.\n",
    "    kstar_mass = tf.broadcast_to(kstar_mass_cast, shape=(n_events,))\n",
    "    if KSTARZ_WIDTH > 0:\n",
    "        kstar_mass = tfp.distributions.TruncatedNormal(loc=kstar_mass,\n",
    "                                                       scale=kstar_width_cast,\n",
    "                                                       low=min_mass,\n",
    "                                                       high=max_mass).sample()\n",
    "    return kstar_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a. Decay chain generation with smeared momenta for the daughter particles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the K*0 and also set its children.\n",
    "kstar = GenParticle('K*0', mass=kstar_mass).set_children(GenParticle('K+', mass=KAON_MASS),\n",
    "                                                         GenParticle('pi-', mass=PION_MASS))\n",
    "bz = GenParticle('B0', B0_MASS).set_children(kstar,\n",
    "                                             GenParticle('mu+', mass=MU_MASS),\n",
    "                                             GenParticle('mu-', mass=MU_MASS))\n",
    "\n",
    "# `generate` returns the weights of the event and a dict which contains the 4-momenta of the particles\n",
    "# as {particle_name: EagerTensor of shape (n_events, 4)}, where an EagerTensor is basically a wrapped NumPy array.\n",
    "# and the particle_name is e.g. 'K+', 'K*0', the name that has been used above.\n",
    "weights, particles = bz.generate(n_sig_rare)\n",
    "weights = weights / np.average(weights)\n",
    "weights = weights.numpy()\n",
    "\n",
    "# Since this is kinematically sampled from a fixed B0 mass, currently the reconstructed invariant B0 mass would only\n",
    "# be a sharp peak. To include detector resolution effects and make it look more like real data/MC, the particles are smeared.\n",
    "smeared_momenta = {}\n",
    "daugther_particles = ['K+', 'pi-', 'mu+', 'mu-']\n",
    "for particle in daugther_particles:\n",
    "    smeared_momenta[particle] = smear_momenta(particles[particle], smearing=rare_smearing)\n",
    "\n",
    "# Reconstruct the mother particles using the smeared daughters.\n",
    "smeared_momenta['K*0'] = smeared_momenta['K+'] + smeared_momenta['pi-']\n",
    "# Reminder - no J/psi in the rare mode. The name 'Jpsi' is simply used for convenience.\n",
    "smeared_momenta['Jpsi'] = smeared_momenta['mu+'] + smeared_momenta['mu-']\n",
    "smeared_momenta['B0'] = smeared_momenta['K*0'] + smeared_momenta['Jpsi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The B0 invariant mass and (mu+,mu-)-pair invariant mass squared are simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_mass_rare = invariant_mass(smeared_momenta['B0'])\n",
    "q2 = invariant_mass(smeared_momenta['Jpsi']) ** 2\n",
    "\n",
    "# Indeed n_sig_rare decays have been generated\n",
    "b_mass_rare.size == q2.size == n_sig_rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.b. Visualisation of the spectrum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-axis label, to be used later.\n",
    "xlabel_bmass = 'm($K^+ \\pi^- \\mu^+ \\mu^-$) [MeV/c^2]'\n",
    "\n",
    "# Plot the B0 mass with the weights. It is basically the same as without weights.\n",
    "plt.figure()\n",
    "plt.title(\"$B^0$ mass generated non resonant\")\n",
    "plt.hist(b_mass_rare, weights=weights, alpha=0.5, bins=40, label=\"with weights\")\n",
    "plt.hist(b_mass_rare, bins=40, alpha=0.5, label=\"no weights\")\n",
    "plt.xlabel(xlabel_bmass)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"$q^2$ generated non resonant\")\n",
    "plt.hist(q2, weights=weights, alpha=0.5, bins=40, label=\"with weights\")\n",
    "plt.hist(q2, bins=40, alpha=0.5, label=\"no weights\")\n",
    "plt.xlabel('$q^2 [MeV^2/c^4]$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Fit to the rare signal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder (see above), the fit of the rare (non-resonant) decay will be done in three steps:\n",
    "- A fit with the (exponential) combinatorial background to the right-hand side of the signal peak, to obtain good starting values for the fitting parameters.\n",
    "- A model composed of an exponential for the combinatorial background and a DoubleCB function for the $B^0 \\to \\mu^+ \\mu^- K^{*0}$ signal is built and fitted to the \"rare\" mode. This is hard to get right, as we set low statistics in the rare mode.\n",
    "- In the next section the resonant mode $B^0 \\to J/\\psi (\\to \\mu^+ \\mu^-) K^{*0}$ will be exploited to improve the fit,\n",
    "  using the same model with a few shared parameters.\n",
    "\n",
    "#### If the fit does not converge\n",
    "\n",
    "Finding the minimum given some parameters is in general not a simple problem. Two major reasons this does\n",
    "not work well are the initial values and the *step size*, which may be too far off or too large/small. The latter\n",
    "should be around the uncertainty of the parameter (once the fit will be converged). More information can be\n",
    "found at https://github.com/zfit/zfit/wiki/FAQ#fitting-and-minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a Fit to the right-hand sideband**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the mass, the observable, defines the range of the data and hence of the fit - [5000, 5600] MeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observable and its range are defined.\n",
    "upper_limit = 5600\n",
    "obs = zfit.Space('Bmass', (5000, upper_limit))  # for whole range\n",
    "obs_bkg = zfit.Space('Bmass', (5400, upper_limit))  # to pre-fit the exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to generate some background data! Indeed only signal has been generated so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are specified:  (name (unique), initial, lower, upper) whereas lower, upper are optional.\n",
    "lambda_rare = zfit.Parameter('lambda_rare', -0.002, -0.01, -0.0001, step_size=0.0005)  # floating, also without limits\n",
    "comb_bkg_rare = zfit.pdf.Exponential(lambda_rare, obs=obs)\n",
    "\n",
    "# Create the background data.\n",
    "comb_bkg_rare_sample = comb_bkg_rare.sample(n=n_bkg_rare)  # sampled within the limits of `obs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer\n",
    "\n",
    "In order to minimize the loss that we will build, a minimizer is needed. zfit wraps multiple libraries such as nlopt, SciPy and Minuit, whereas the latter is quite succesfully used in HEP and tends to be the most reliable and stable overall. Due to the unified API, they can just be exchanged with each other.\n",
    "\n",
    "More information and minimizers can be [found here](https://zfit.readthedocs.io/en/latest/getting_started/intro/minimize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer = zfit.minimize.Minuit(verbosity=1, gradient=True)  # gradient=True: use minuit gradient, \n",
    "                                                              # can be more stable\n",
    "# minimizer = zfit.minimize.NLoptLBFGSV1(verbosity=1)\n",
    "# minimizer = zfit.minimize.ScipyTrustConstrV1(verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, the right-hand side of the data, which is only the combinatorial background freshly generated, is fitted to the exponential model in order to get a good initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_tale_data_rare = zfit.Data.from_numpy(obs=obs_bkg, array=comb_bkg_rare_sample.value())\n",
    "\n",
    "# Set the value of lambda to something different than we sampled from (for the fit afterwards).\n",
    "lambda_rare.set_value(-0.003)\n",
    "\n",
    "# The normalisation range is temporarily set to the right-hand side only.\n",
    "with comb_bkg_rare.set_norm_range(obs_bkg):\n",
    "    right_tale_loss = zfit.loss.UnbinnedNLL(comb_bkg_rare, right_tale_data_rare)\n",
    "    result_right_tale = minimizer.minimize(right_tale_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_right_tale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b Fit to the full spectrum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data for the rare fit\n",
    "rare_data_np = np.concatenate([b_mass_rare, comb_bkg_rare_sample[:, 0]], axis=0)\n",
    "rare_weights_np = np.concatenate([weights, np.ones_like(comb_bkg_rare_sample[:, 0])], axis=0)\n",
    "# TODO: we could do some data preprocessing here (e.g. apply a cut to q2, which we actually\n",
    "# need to have \"no Jpsi\" in there). This can be done with pandas and then loaded into zfit\n",
    "\n",
    "rare_data = zfit.Data.from_numpy(obs=obs, array=rare_data_np, weights=rare_weights_np)\n",
    "# Data can also be loaded from ROOT\n",
    "# right_tale_data_rare = zfit.Data.from_root(...)\n",
    "\n",
    "# ...or to/from a pandas DataFrame. Either convert the `zfit.Data` to a pandas DF\n",
    "# right_tale_data_rare_df = right_tale_data_rare.to_pandas()\n",
    "# .... or create it from scratch\n",
    "# right_tale_data_rare_df = pd.DataFrame(data=rare_data_np, columns=obs_bkg.obs)\n",
    "# Then we can directly load it\n",
    "# right_tale_data_rare = zfit.Data.from_pandas(df=right_tale_data_rare_df, obs=obs_bkg)\n",
    "\n",
    "# (maybe remove plot, just for data visualisation example?)\n",
    "plt.figure()\n",
    "plt.title(\"Invariant mass of combinatorial bkg and signal $B^0 -> K^{*} (-> K^+ \\pi^-) \\mu^+ \\mu^-$\")\n",
    "plt.hist(rare_data_np, weights=rare_weights_np, bins=40)\n",
    "plt.xlabel(xlabel_bmass)\n",
    "\n",
    "# Create the model to fit to the rare mode\n",
    "\n",
    "# Parameters for the model\n",
    "mu = zfit.Parameter('mu', 5270, 5200, 5350, step_size=3)\n",
    "sigma = zfit.Parameter('sigma', 22, 1, 100, step_size=0.1)\n",
    "\n",
    "# A double crystalball, a sum of two Crystal Ball functions will be used\n",
    "alphal_rare = zfit.Parameter('alpha left rare', -0.3, -5, 0)\n",
    "nl_rare = zfit.Parameter('n left rare', 0.3, 0, 10)\n",
    "alphar_rare = zfit.Parameter('alpha right rare', 1, 0, 5)\n",
    "nr_rare = zfit.Parameter('n right rare', 2.8, 0, 10)\n",
    "frac_dcb_rare = zfit.Parameter('frac dcb', 0.3, 0.1, 0.9)\n",
    "\n",
    "left_cb_rare = zfit.pdf.CrystalBall(obs=obs,\n",
    "                                    mu=mu, sigma=sigma,\n",
    "                                    alpha=alphal_rare, n=nl_rare,\n",
    "                                    )\n",
    "right_cb_rare = zfit.pdf.CrystalBall(obs=obs,\n",
    "                                     mu=mu, sigma=sigma,\n",
    "                                     alpha=alphar_rare, n=nr_rare,\n",
    "                                     )\n",
    "signal_rare = zfit.pdf.SumPDF([left_cb_rare, right_cb_rare], fracs=frac_dcb_rare)\n",
    "\n",
    "\n",
    "# Create the yields and the extended pdfs\n",
    "rare_sig_yield = zfit.Parameter('rare_sig_yield', n_sig_rare + 30,\n",
    "                                step_size=3)  # step size: default is small, use appropriate\n",
    "rare_bkg_yield = zfit.Parameter('rare_bkg_yield', n_bkg_rare - 40, step_size=1)\n",
    "# Create extended PDFs\n",
    "extended_sig_rare = signal_rare.create_extended(rare_sig_yield)\n",
    "extended_bkg_rare = comb_bkg_rare.create_extended(rare_bkg_yield)\n",
    "\n",
    "# The final model is the combination of the signal and backgrond pdf\n",
    "model_rare = zfit.pdf.SumPDF([extended_sig_rare, extended_bkg_rare])\n",
    "\n",
    "\n",
    "# Different models could be used here. A more simple model, such as a Gaussian...\n",
    "# signal_rare = zfit.pdf.Gauss(mu=mu, sigma=sigma, obs=obs)\n",
    "\n",
    "# ...or even a completely custom PDF. Explained also here: \n",
    "# https://zfit.readthedocs.io/en/latest/getting_started/intro/model.html#custom-pdf\n",
    "# from zfit import z  # backend, can also use TensorFlow directly\n",
    "#\n",
    "# class MySignal(zfit.ZPDF):\n",
    "#     _N_OBS = 1\n",
    "#     _PARAMS = ['mean', 'width']\n",
    "#    \n",
    "#     # Implement the shape (unnormalized) of your pdf. If the analytical integral is known, it can be\n",
    "#     # registered later on.\n",
    "#     def _unnormalized_pdf(self, x):\n",
    "#         x = z.unstack(x)  # could be higher dimensional\n",
    "#         mean = self.params['mean']\n",
    "#         width = self.params['width']\n",
    "#         return z.exp((x - mean) ** 2 / (2 * width ** 2))  # Gaussian, unnormalized\n",
    "#\n",
    "#\n",
    "# signal_rare = MySignal(mean=mu, width=sigma, obs=obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit to the signal simulation**\n",
    "\n",
    "If we have simulation available, it can be good to fit the signal PDF to the simulation first in order to fix more sensitive parameters such as the tails of a Crystalball. Otherwise, these tend to cause instabilities if they are let floating in the whole fit.\n",
    "\n",
    "Therefore we fit first the simulation (which we skip here) and then fix the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_mass_rare_sim = zfit.Data.from_...  # shortcut, in reality we would load the simulation here\n",
    "# nll_signal_sim = zfit.loss.UnbinnedNLL(model=signal_rare, data=b_mass_rare_sim)\n",
    "# minimizer.minimize(nll_signal_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nl_rare.floating = False\n",
    "nr_rare.floating = False\n",
    "alphal_rare.floating = False\n",
    "alphar_rare.floating = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c. Visualisation of the spectrum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to plot our model, a helper function is created\n",
    "def plot_pdf_data(data, model, title, n_bins=40):\n",
    "    linewidth = 2.5\n",
    "    space = data.data_range\n",
    "    plot_scaling = data.nevents / n_bins * space.area()\n",
    "    lower, upper = space.limit1d\n",
    "    x = np.linspace(lower, upper, 1000)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "\n",
    "    # plot the data\n",
    "    data_np = data[:, 0]\n",
    "    plt.hist(data_np,\n",
    "             # color=color,\n",
    "             bins=n_bins, histtype=\"stepfilled\", alpha=0.1)\n",
    "    plt.hist(data_np,\n",
    "             # color=color,\n",
    "             bins=n_bins, histtype=\"step\")\n",
    "    # plot the pdfs\n",
    "    y = zfit.run(model.pdf(x))\n",
    "    y_sig = zfit.run(model.pdfs[0].pdf(x) * model.fracs[0])# notice the frac!\n",
    "    y_bkg = zfit.run(model.pdfs[1].pdf(x) * model.fracs[1])  # notice the frac!\n",
    "\n",
    "    plt.plot(x, y * plot_scaling, label=\"Sum - Model\", linewidth=linewidth * 2)\n",
    "    plt.plot(x, y_sig * plot_scaling, '--', label=f\"{model.pdfs[0].name} - Signal\", linewidth=linewidth)\n",
    "    plt.plot(x, y_bkg * plot_scaling, '--', label=f\"{model.pdfs[1].name} - Background\", linewidth=linewidth)\n",
    "    plt.xlabel(space.obs[0])\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "plot_pdf_data(data=rare_data, model=model_rare, title='before fitting: $B^0 -> K^{*} (-> K^+ \\pi^-) \\mu^+ \\mu^-$')\n",
    "plt.show()\n",
    "\n",
    "# As a next step, the loss is created: the minimum of it defines the solution to the problem\n",
    "ext_rare_nll = zfit.loss.ExtendedUnbinnedNLL(model_rare, rare_data)\n",
    "\n",
    "# The minimization uses by default all floating parameters (that the model(s) used in the loss depend on)\n",
    "# but they can also be explicitly specified as an argument to `minimize`.\n",
    "result_rare = minimizer.minimize(ext_rare_nll)\n",
    "\n",
    "# The parameter values are set automatically to the value found at the minimum.\n",
    "plot_pdf_data(data=rare_data, model=model_rare, title='after rare fitting: $B^0 -> K^{*} (-> K^+ \\pi^-) \\mu^+ \\mu^-$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information about the minimization and the result of it are stored in the\n",
    "# FitResult that is returned. It contains e.g. information about the parameters\n",
    "pprint(result_rare.params)\n",
    "\n",
    "# ...and more:\n",
    "print(f\"The fit converged: {result_rare.converged}, the minimum is {result_rare.fmin}\")\n",
    "\n",
    "# Following are error estimations. They return the result and also add it into the `params` attribute.\n",
    "with zfit.run.set_autograd_mode(False):  # this will run the error calculation with numerical methods\n",
    "    result_rare.hesse()  # error calculation using the inverse hessian approximation\n",
    "\n",
    "# Using errors as below does only work for fits without weights!\n",
    "# result_rare.errors()  # error calculation using minos, this takes all parameters (expensive)\n",
    "# result_rare.errors([rare_sig_yield, mu])  # just for specific parameters\n",
    "pprint(result_rare.params)\n",
    "\n",
    "# the params can be accesssed using the parameter objects (not their names!)\n",
    "mu_rare_fit = result_rare.params[mu]\n",
    "# they contain information about the result such as value etc\n",
    "print(f\"Mu value of rare fit: {mu_rare_fit['value']} \"\n",
    "#       f\"+ {mu_rare_fit['minuit_minos']['upper']} \"\n",
    "#       f\"- {mu_rare_fit['minuit_minos']['upper']}\"\n",
    "      f\" (symmetric Hesse error: {mu_rare_fit['minuit_hesse']['error']})\"\n",
    "#       f\" (symmetric Hesse error: {mu_rare_fit['hesse_np']['error']})\"  # if not using Minuit\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a nice overview, we can also print the parameters (or result) directly\n",
    "print(result_rare.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Phasespace generation of the resonant decay**\n",
    "\n",
    "Since the previous fit, e.g. the tail, was not great due to the lack of statistics, we can also fit the resonant mode simultaneously and share certain parameters.\n",
    "\n",
    "**3.a. Signal decay chain generation with smeared momenta for the daughter particles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resonant decay. This is basically the same as above, we simply add an intermediate Jpsi.\n",
    "bz = GenParticle('B0', B0_MASS).set_children(kstar,\n",
    "                                             GenParticle('Jpsi', mass=JPSI_MASS).set_children(\n",
    "                                                 GenParticle('mu+', mass=MU_MASS),\n",
    "                                                 GenParticle('mu-', mass=MU_MASS)\n",
    "                                             ))\n",
    "\n",
    "weights_reso, particles_reso = bz.generate(n_sig_reso)\n",
    "weights_reso /= np.average(weights_reso)\n",
    "weights_reso = weights_reso.numpy()\n",
    "\n",
    "smeared_momenta_reso = {}\n",
    "daugther_particles_reso = ['K+', 'pi-', 'mu+', 'mu-']\n",
    "for particle in daugther_particles_reso:\n",
    "    smeared_momenta_reso[particle] = smear_momenta(particles_reso[particle], smearing=reso_smearing)\n",
    "\n",
    "smeared_momenta_reso['K*0'] = smeared_momenta_reso['K+'] + smeared_momenta_reso['pi-']\n",
    "smeared_momenta_reso['Jpsi'] = smeared_momenta_reso['mu+'] + smeared_momenta_reso['mu-']\n",
    "smeared_momenta_reso['B0'] = smeared_momenta_reso['K*0'] + smeared_momenta_reso['Jpsi']\n",
    "\n",
    "b_mass_reso = invariant_mass(smeared_momenta_reso['B0'])\n",
    "q2_reso = invariant_mass(smeared_momenta_reso['Jpsi'])\n",
    "\n",
    "# plot the q2\n",
    "plt.figure()\n",
    "plt.title(\"$q^2$ generated resonant\")\n",
    "plt.hist(q2_reso, weights=weights_reso, alpha=0.5, bins=40, label=\"with weights\")\n",
    "plt.hist(q2_reso, bins=40, alpha=0.5, label=\"no weights\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.b. Combinatorial background generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reso = zfit.Parameter('lambda_reso', -0.002, -0.01, 0.0001)  # floating, also without limits\n",
    "comb_bkg_reso_pdf = zfit.pdf.Exponential(lambda_reso, obs=obs)\n",
    "\n",
    "# Create some background data\n",
    "comb_bkg_reso_sample = comb_bkg_reso_pdf.sample(n=n_bkg_reso)  # sampled within the limits of `obs`\n",
    "\n",
    "# Set the value of lambda to smth different then we sampled from (for the fit afterwards)\n",
    "lambda_reso.set_value(-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.c. Signal + background data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reso_data_np = np.concatenate([b_mass_reso, comb_bkg_reso_sample[:, 0]], axis=0)\n",
    "reso_weights_np = np.concatenate([weights_reso, np.ones_like(comb_bkg_reso_sample[:, 0])], axis=0)\n",
    "\n",
    "reso_data = zfit.Data.from_numpy(obs=obs, array=reso_data_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.d. Visualisation of the full spectrum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (maybe remove plot, just for data visualization example?)\n",
    "plt.figure()\n",
    "plt.title(\"Invariant B mass $B^0 -> K^{*} (-> K^+ \\pi^-) J/\\psi (-> \\mu^+ \\mu^-)$\")\n",
    "plt.xlabel(xlabel_bmass)\n",
    "plt.hist(reso_data_np, weights=reso_weights_np, bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.e Model for the resonant decay**\n",
    "\n",
    "Create the full model to fit the spectrum of the resonant signal model and its related combinatorial background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can share parameters directly or create composed parameters. Here we have a\n",
    "# parameter that scales the sigma from the rare fit\n",
    "\n",
    "sigma_scaling = zfit.Parameter('sigma_scaling', 0.9, 0.5, 10, step_size=0.1)\n",
    "\n",
    "\n",
    "def sigma_scaled_fn():\n",
    "    return sigma * sigma_scaling  # this can be an arbitrary function\n",
    "\n",
    "\n",
    "sigma_scaled = zfit.ComposedParameter('sigma scaled', sigma_scaled_fn,\n",
    "                                      dependents=sigma  # the objects used inside the function\n",
    "                                      )\n",
    "\n",
    "# Instead of sharing, free parameters could be used. This would though defy the purpose of the simultaneous fit here.\n",
    "# If we know that some can't be shared though (e.g. the tails between a muon and electron modes), only certain parameters can be shared.\n",
    "# alphal_reso = zfit.Parameter('alpha left reso', -0.7, -5, 0)\n",
    "# nl_reso = zfit.Parameter('n left reso', 0.4, 0, 10)\n",
    "# alphar_reso = zfit.Parameter('alpha right reso', 1, 0, 5)\n",
    "# nr_reso = zfit.Parameter('n right reso', 1.8, 0, 10)\n",
    "alphal_reso = alphal_rare\n",
    "nl_reso = nl_rare\n",
    "alphar_reso = alphal_rare\n",
    "nr_reso = nr_rare\n",
    "\n",
    "# since we share the tails now, we expect this to be stable and we can let them float\n",
    "alphal_reso.floating = True\n",
    "alphar_reso.floating = True\n",
    "nl_reso.floating = True\n",
    "nr_reso.floating = True\n",
    "\n",
    "# frac_dcb_reso = zfit.Parameter('frac dcb_reso', 0.5, 0.01, 0.99)\n",
    "frac_dcb_reso = frac_dcb_rare\n",
    "left_cb_reso = zfit.pdf.CrystalBall(obs=obs,\n",
    "                                    mu=mu, sigma=sigma_scaled,\n",
    "                                    alpha=alphal_reso, n=nl_reso,\n",
    "                                    )\n",
    "right_cb_reso = zfit.pdf.CrystalBall(obs=obs,\n",
    "                                     mu=mu, sigma=sigma_scaled,\n",
    "                                     alpha=alphar_reso, n=nr_reso,\n",
    "                                     )\n",
    "signal_reso = zfit.pdf.SumPDF([left_cb_reso, right_cb_reso], fracs=frac_dcb_reso)\n",
    "\n",
    "# Again, also a simpler (or custom) shape could be used\n",
    "# signal_reso = zfit.pdf.Gauss(mu=mu,  # using the same mu as above means it's shared\n",
    "#                              sigma=sigma_scaled, obs=obs)\n",
    "\n",
    "reso_sig_yield = zfit.Parameter('reso_sig_yield', n_sig_reso - 100, 0, n_sig_reso * 3,\n",
    "                                step_size=1)  # step size: default is small, use appropriate\n",
    "reso_bkg_yield = zfit.Parameter('reso_bkg_yield', n_bkg_reso + 70, 0, n_bkg_reso * 3, step_size=1)\n",
    "\n",
    "# Create the extended models\n",
    "extended_sig_reso = signal_reso.create_extended(reso_sig_yield)\n",
    "extended_bkg_reso = comb_bkg_reso_pdf.create_extended(reso_bkg_yield)\n",
    "model_reso = zfit.pdf.SumPDF([extended_bkg_reso, extended_sig_reso])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Simultaneous fit**\n",
    "\n",
    "- Built the simultaneous loss function.\n",
    "- Run the simultaneous fit.\n",
    "- Print and plot the fit results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraints could also be added to the loss, we don't have a good use-case here, so we leave them away.\n",
    "# They usually resemble other measurements of a parameter with an uncertainty in the same order of\n",
    "# magnitude as the sensitivity of our fit. Too strong constraint parameters can be though of as constants,\n",
    "# too loose do not add anything.\n",
    "# constraint = zfit.constraint.GaussianConstraint(mu, observation=5279, uncertainty=50)\n",
    "# ext_reso_nll = zfit.loss.ExtendedUnbinnedNLL(model_reso, reso_data, constraints=constraint)\n",
    "\n",
    "ext_reso_nll = zfit.loss.ExtendedUnbinnedNLL(model_reso, reso_data)\n",
    "\n",
    "# to create a simultaneous loss, the individual losses are added\n",
    "simultaneous_loss = ext_reso_nll + ext_rare_nll\n",
    "\n",
    "result_simult = minimizer.minimize(simultaneous_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_simult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf_data(data=rare_data, model=model_rare, title='$B^0 -> K^{*} (-> K^+ \\pi^-) \\mu^+ \\mu^-$')\n",
    "plot_pdf_data(data=reso_data, model=model_reso, title='$B^0 -> K^{*} (-> K^+ \\pi^-) J/\\psi (-> \\mu^+ \\mu^-)$')\n",
    "plt.show()\n",
    "\n",
    "with zfit.run.set_autograd_mode(False):\n",
    "    result_simult.hesse(params=[reso_bkg_yield, reso_sig_yield], method='minuit_hesse')  # error calculation using hesse. This corrects for the presence of weights\n",
    "# Using errors as below does only work for fits without weights!\n",
    "# errors, _ = result_simult.errors([mu, rare_sig_yield, reso_sig_yield])  # error calculation using minos, just for a few\n",
    "# parameters as it is quite expensive\n",
    "print(result_simult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Wrapping up - determination of the signal significance**\n",
    "\n",
    "Going beyond the fits one is often interested in obtaining limits or calculate signal significances.\n",
    "These statistical procedures usually perform multiple fits with certain parameters fixed.\n",
    "\n",
    "For this purpose, the `hepstats` library can be used. It is built on top of the same interface that `zfit`\n",
    "uses and therefore integrates well together. Models built and fits done in `zfit` will directly work with `hepstats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepstats.hypotests import Discovery\n",
    "from hepstats.hypotests.calculators import AsymptoticCalculator\n",
    "from hepstats.hypotests.parameters import POI\n",
    "\n",
    "calculator = AsymptoticCalculator(simultaneous_loss, minimizer)\n",
    "poinull = POI(rare_sig_yield, 0)\n",
    "discovery_test = Discovery(calculator, poinull)\n",
    "pnull, significance = discovery_test.result()\n",
    "print(f'pnull: {pnull} with significance {significance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
