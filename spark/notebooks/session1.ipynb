{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ESCAPE Summer School 2021: Big data for big science #1\n",
    "\n",
    "<img src=\"../pictures/spark_escape_logo.png\" alt=\"alt text\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who am I?\n",
    "\n",
    "- Julien Peloton\n",
    "- 2015: PhD in cosmology (APC/CNRS)\n",
    "- 2018-now: Research software engineer at IJCLab/CNRS\n",
    "- Main technical activities: big data processing, parallel computing, cloud computing\n",
    "- Main scientific activities: Transient sky (Rubin Observatory) and multi-messenger astronomy, Cosmic Microwave Background\n",
    "- https://github.com/JulienPeloton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before we start\n",
    "\n",
    "- The material can be found in the GitHub repo of the school, folder `spark/`\n",
    "- All notebooks and exercises can be run in a Docker container (preferred). See the `README.md`.\n",
    "- You can also run notebooks on Google Colab (see instructions inside notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you are running this notebook on Google Colab, you need to install Spark before. As we are working in local mode (i.e. there is only one Java Virtual Machine which hosts both driver and executor threads) we can simply install it using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment these lines if you are using Google Colab\n",
    "# !pip install pyspark==3.1.1\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialise our Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "The main goal of this lecture is to get familiar with Apache Spark, and in particular its Python API called PySpark in a scientific context. In this first notebook, we will introduce few Apache Spark functionalities of interest (and by no means complete!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Apache Spark\n",
    "\n",
    "<img src=\"../pictures/spark_timeline.png\" alt=\"alt text\"/>\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/) is a cluster computing framework, that is a set of tools to perform computation on a network of many machines. Spark started in 2009 as a research project, and it had a huge success so far in the industry. It is based on the so-called **MapReduce** cluster computing paradigm, popularized by the Hadoop framework using implicit data parallelism and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When using Apache Spark?\n",
    "\n",
    "<img src=\"../pictures/spark_logo.png\" alt=\"alt text\" width=\"200\" align=\"right\"/>\n",
    "\n",
    "- When you have a LOT of data (static, or streaming)\n",
    "- When you need to perform iterative analyses over a large dataset\n",
    "- When you want to perform interactive data analysis over a large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why using Apache Spark?\n",
    "\n",
    "<img src=\"../pictures/spark_logo.png\" alt=\"alt text\" width=\"200\" align=\"right\"/>\n",
    "\n",
    "- Unified analytics engine for large-scale data processing: all in one!\n",
    "- Open source\n",
    "- Large and active community of developers and users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What are the main features?\n",
    "\n",
    "<img src=\"../pictures/spark_logo.png\" alt=\"alt text\" width=\"200\" align=\"right\"/>\n",
    "\n",
    "- Implicit data parallelism\n",
    "- Easily scalable from a laptop to entire clusters\n",
    "- Fault tolerance\n",
    "- Functional programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../pictures/scala_logo.png\" alt=\"alt text\" width=\"200\"/>\n",
    "\n",
    "The core of Spark is written in Scala which is a general-purpose programming language that has been started in 2004 by Martin Odersky (EPFL). The language is inter-operable with Java and Java-like languages, and Scala executables run on the Java Virtual Machine (JVM). While Scala contains many ideas from functional programming, it is multi-paradigm, including functional programming, imperative programming, object-oriented programming and concurrent computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark provides many functionalities exposed through Scala/Python/Java/R API (Scala being the native one). As far as this lecture is concerned, we will use the Python API (called PySpark). But feel free to put your hands on Scala, it's worth it. For those interested, you can have a look at this [tutorial](https://gitlab.in2p3.fr/MaitresNageurs/QuatreNages/Scala) on Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning objectives for session 1\n",
    "\n",
    "- Loading and distributing data with Spark SQL (Apache Spark Data Sources API)\n",
    "- Exploring DataFrame & partitioning\n",
    "- Manipulating Spark SQL built-in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributed computing\n",
    "\n",
    "<img src=\"../pictures/spark_cluster.png\" alt=\"alt text\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Spark cluster\n",
    "\n",
    "Spark runs on clusters of machines, and typically in a cloud environment. In a simple deploy mode, a cluster is made of a driver node, connected via ssh to several worker nodes. Each worker is typically linked to a distributed file system (HDFS, Ceph, Amazon S3, ...). Two execution modes: _client_ or _cluster_ mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Resource Scheduling\n",
    "\n",
    "Resource scheduling and optimisation can be a difficult operation if done manually (the standalone mode of Spark). Alternatively, Spark can run on clusters managed by cluster managers such as YARN, Mesos, or even Kubernetes. See https://spark.apache.org/docs/latest/#launching-on-a-cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Running applications step-by-step\n",
    "\n",
    "1. `Driver` contacts the `cluster manager` and requests for resources to launch the `Executors` inside the `Workers`.\n",
    "2. The `cluster manager` launches the `Executors` on behalf of the `Driver`.\n",
    "3. Once the `Executors` are launched, they establish a direct connection with the `Driver`.\n",
    "4. The `Driver` determines the total number of `Tasks` by checking the `Lineage`.\n",
    "5. The `Driver` creates the `Logical` and `Physical Plan`.\n",
    "6. Once the `Physical Plan` is generated, Spark allocates the `Tasks` to the `Executors`.\n",
    "7. `Task` runs on `Executor` and each `Task` upon completion returns the result to the `Driver`.\n",
    "8. Finally, when all `Tasks` are completed, the program exits, and Spark releases all the resources from the `Cluster Manager`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to run Spark applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "# Launching a script\n",
    "spark-submit [OPTIONS] mysparkscript.py [OPTIONS]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "# Launching a python shell (ipython)\n",
    "PYSPARK_DRIVER_PYTHON=ipython pyspark [OPTIONS]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "# Launching a jupyter-notebook\n",
    "PYSPARK_DRIVER_PYTHON=jupyter-notebook pyspark [OPTIONS]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "There are 4 main libraries in Apache Spark:\n",
    "1. **SQL & DataFrames**\n",
    "2. Streaming & Structured Streaming\n",
    "3. MLlib (Machine Learning)\n",
    "4. GraphX (graph)\n",
    "\n",
    "For this lecture, we will focus on the SQL & DataFrames library. This library lets you query structured data inside Apache Spark programs, using either SQL or a familiar DataFrame API (Ã  la Pandas). Note that the other libraries are also using the functionnalities of the SQL & DataFrames one (unified interface)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Spark Data Sources\n",
    "\n",
    "### A tour of data formats\n",
    "\n",
    "- Many data formats used in the context of Big Data: CSV (1978), XML (1996), JSON (2001), Thrift (2007), Protobuf (2008), Avro & SequenceFile (2009), Parquet (2013), ORC (2016), and the list goes on... \n",
    "- Some are _naively_ structured versus more complex and highly optimised for big data treatment (e.g. Parquet). \n",
    "\n",
    "Unfortunately those are often not the data formats typically chosen by the scientific community, e.g.: FITS (1981), HDF5 (1988) format, or ROOT (1995). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Connecting to Data Source\n",
    "\n",
    "The data source API in Apache Spark belongs to the [Spark SQL module](https://spark.apache.org/sql/). Note that Spark Core has some simple built-in ways to read data from disk (binary or text), but Spark SQL is more complete and give you access to DataFrames directly. If you want to connect a specific data source with Apache Spark:\n",
    "\n",
    "- [indirect] Access and distribute your files as binary streams (Spark does it natively), and decode the data on-the-fly within executors using third-party libraries --> _Easy to do, but poor performance._\n",
    "- [native] Use a built-in or custom connector to access, distribute and decode the data natively --> _More challenging to write, but excellent performance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Connectors for scientific data formats?\n",
    "\n",
    "By default, Spark cannot read natively most scientific formats like FITS, HDF5, or ROOT. There were several attempts though to build connectors: \n",
    "\n",
    "[1] Z. Zhang et al, Kira: Processing Astronomy Imagery Using Big Data Technology, DOI 10.1109/TBDATA.2016.2599926.    \n",
    "[2] Peloton et al, FITS Data Source for Apache Spark, Computing and Software for Big Science (1804.07501). https://github.com/astrolabsoftware/spark-fits   \n",
    "[3] Liu et al, H5spark: bridging the I/O gap between Spark and scientific data formats on HPC systems, Cray user group (2016). https://github.com/valiantljk/h5spark  \n",
    "[4] Viktor Khristenko, & Jim Pivarski. (2017, October 20). diana-hep/spark-root: Release 0.1.14 (Version v0.1.14). Zenodo. http://doi.org/10.5281/zenodo.1034230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Most of scientific data formats, were not designed for serialisation (distribution of data over machines) originally and they often use compression to reduce the size on disk. Needless to say that default Spark cannot read those natively.\n",
    "\n",
    "First attempts to connect those data formats (see e.g. [1] for FITS) with Spark were using the indirect method above. By reading files as binary streams, the indirect method has the advantage of having access to all FITS functionalities implemented in the underlying user library. This can be an advantage when working with the Python API for example which already contains many great scientific libraries. However this indirect method assumes each Spark mapper will receive and handle one entire file (since the filenames are parallelized and entire file data must be reconstructed from binary once the file has been opened by a Spark mapper). Therefore each single file must fit within the memory of a Spark mapper, hence the indirect method cannot distribute a dataset made of large FITS files (e.g. in [1] they have a 65 GB dataset made of 11,150 files). In addition by assuming each Spark mapper will receive and handle one entire file, the indirect method will have a poor load balancing if the dataset is made of files with not all the same size.\n",
    "\n",
    "Fortunately Apache Spark low-level layers are sufficiently well written to allow extending the framework and write native connectors for any kind of data sources. Recently connectors for FITS, HDF5 and ROOT were made available [2, 3, 4] to the community. With such connectors, there is a guarantee of having a good load balancing regardless the structure of the dataset and the size of the input files is no more a problem (a 1 TB dataset made of thousand 1 GB files or one single 1 TB file will be viewed as almost the same by a native Spark connector). Note however that the Data Source API is in Java/Scala and if there is no library to play with your data source in those languages you must implement it (what has been done in [2]) or interface with another language.\n",
    "\n",
    "Note that the low-level layers dealing with the data sources have been recently updated. Apache Spark 2.3 introduced the Data Source API version 2. While the version 1 is still available and usable for a long time, we expect that all Spark connectors will comply with this v2 in the future.\n",
    "\n",
    "[1] Z. Zhang and K. Barbary and F. A. Nothaft and E. R. Sparks and O. Zahn and M. J. Franklin and D. A. Patterson and S. Perlmutter, Kira: Processing Astronomy Imagery Using Big Data Technology, DOI 10.1109/TBDATA.2016.2599926.  \n",
    "[2] Peloton, Julien and Arnault, Christian and Plaszczynski, StÃ©phane, FITS Data Source for Apache Spark, Computing and Software for Big Science (1804.07501). https://github.com/astrolabsoftware/spark-fits   \n",
    "[3] Liu, Jialin and Racah, Evan and Koziol, Quincey and Canon, Richard Shane, H5spark: bridging the I/O gap between Spark and scientific data formats on HPC systems, Cray user group (2016). https://github.com/valiantljk/h5spark  \n",
    "[4] Viktor Khristenko, & Jim Pivarski. (2017, October 20). diana-hep/spark-root: Release 0.1.14 (Version v0.1.14). Zenodo. http://doi.org/10.5281/zenodo.1034230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark hands-on: entry point\n",
    "\n",
    "By default, this notebook comes with several instantiated objects to interact with the Spark cluster you just created by launching this notebook. The most important is the Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c519630ee1d7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa99d2a5790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark Session will allow you to read data, and manage the configuration of the cluster. Note you can also create it (or get the current one running) via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataFrameReader\n",
    "\n",
    "The interface to read data from disk is always the same for any kind of built-in and officially supported data format:\n",
    "\n",
    "```python\n",
    "df = spark.read\\\n",
    "    .format(format: str)\\\n",
    "    .option(key: str, value: Any)\\\n",
    "    # ...\n",
    "    .option(key: str, value: Any)\\\n",
    "    .load(path: str)\n",
    "```\n",
    " \n",
    "Note that for most of the data sources, you can use wrappers such as:\n",
    "\n",
    "```python\n",
    "spark.read.csv(path, key1=value1, key2=value2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Format**: The format can be \"csv\", \"json\", \"parquet\", etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Options**: The number of options depends on the underlying data source. Each has its own set of options. \n",
    "In most of the case, no options are needed, but you might want to explore the different possibilities at some point. Surprisingly it is not easy to find documentation and the best remains to read the source code documentation. In pyspark you can easily access it via the wrappers:\n",
    "\n",
    "```python\n",
    "# DataFrameReader object\n",
    "df_reader = spark.read\n",
    "\n",
    "# Doc on reading CSV\n",
    "df_reader.csv?\n",
    "# doc printed\n",
    "\n",
    "# Doc on reading Parquet\n",
    "df_reader.parquet?\n",
    "# doc printed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Path**: The way to specify path is threefold: either a single file (`path/to/folder/myfile.source`), or an entire folder (`path/to/folder`), or a glob pattern (`path/to/folder/*pattern*.source`). Note that you also need to specify the type of file system you are using. Example:\n",
    "\n",
    "``` python\n",
    "# Connect to hdfs\n",
    "path = 'hdfs:///path/to/data'\n",
    "\n",
    "# Connect to S3\n",
    "path = 's3:///path/to/data'\n",
    "\n",
    "# Connect to local file system\n",
    "path = 'files:///path/to/data'\n",
    "```\n",
    "\n",
    "If nothing is specified (`'/path/to/data'`), it will adapt to your `--master` (e.g. if you launch spark in local mode, you will connect to the local file system by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Specifying dependencies (Scala/Java)\n",
    "\n",
    "You can also connect to custom connector not included in the default Spark distribution. If you are using Pyspark, you will need to specify the external dependencies when submitting your job or invoking your shell. If your connector is available through [Maven Central Repository](https://search.maven.org/), you can easily specify it via:\n",
    "\n",
    "```bash\n",
    "# Direct download from central repository\n",
    "spark-submit --packages groupId:artifactId:version ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that this is the same syntax when launching the `pyspark` shell.\n",
    "For example, if you want to read FITS files using the [spark-fits](https://github.com/astrolabsoftware/spark-fits) connector you would add the following:\n",
    "\n",
    "```bash\n",
    "# Direct download from central repository\n",
    "spark-submit --packages com.github.astrolabsoftware:spark-fits_2.12:0.9.0 ...\n",
    "```\n",
    "\n",
    "You can find the spark-fits entry in the Maven Central [here](https://search.maven.org/artifact/com.github.astrolabsoftware/spark-fits_2.12/0.9.0/jar) for reference.\n",
    "Alternatively you can download the source code for a particular connector, compile it and include the `jars`:\n",
    "\n",
    "```bash\n",
    "# Specify manually the dependency\n",
    "spark-submit --jars /path/to/lib/spark-fits.jars ...\n",
    "```\n",
    "\n",
    "Note that when you launch `pyspark`, already a numbers of `jars` are included by default (the ones for Spark for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading and distributing data\n",
    "\n",
    "You will find test data in the folder `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some historical consideration\n",
    "\n",
    "The main data object in Spark is the `DataFrame`. It was not always the case. Spark started with the concept of `RDD`. RDDs are distributed memory abstractions, fault-tolerant and immutable. They are designed for a general reuse (e.g. performing interactive data mining).\n",
    "\n",
    "A `DataFrame` is a distributed collection of records, like a `RDD`, organized into named columns and including the benefits of Spark SQLâs execution engine. This is a new interface added in Spark version 1.6. Of course, the `RDD` interface is still accessible, and sometimes very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading Data: simply structured data (text)\n",
    "\n",
    "You can load CSV data into a DataFrame by simply using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load simple CSV file\n",
    "df_csv_simple = spark.read.format(\"csv\")\\\n",
    "    .load(\"../data/clusters.csv\")\n",
    "df_csv_simple.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice by default the CSV connector interprets all entries as String, and give dummy names to columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** How would you infer the data type and use the first row as column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No schema information\n",
    "df_csv_simple = spark.read.format(\"csv\")\\\n",
    "    .load(\"../data/clusters.csv\")\n",
    "\n",
    "# Add schema information\n",
    "df_csv_with_schema = spark.read.format(\"csv\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load(\"../data/clusters.csv\")\n",
    "\n",
    "df_csv_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+---+\n",
      "|                   x|                  y|                 z| id|\n",
      "+--------------------+-------------------+------------------+---+\n",
      "| -1.4076402686194887|  6.673344773733206| 8.208460943517498|  2|\n",
      "|  0.6498424376672443|  3.744291410605022|1.0917784706793445|  0|\n",
      "|  1.3036201950328201|-2.0809475280266656| 4.704460741202294|  1|\n",
      "| -1.3741641126376476|  4.791424573067701| 2.562770404033503|  0|\n",
      "|  0.3454761504864363| -2.481008091382492|2.3088066072973583|  1|\n",
      "|  -2.108033815295204|  7.894891387339489| 9.595407530206733|  2|\n",
      "|   2.840793398208305| -2.148969881643252|1.4240818259724246|  1|\n",
      "| -0.5731983175929686|  8.670542120889035|10.758730552494102|  2|\n",
      "|  2.7096812101578305|-3.6348733373730777| 3.270643002860278|  1|\n",
      "|-0.23472121261093792|  7.497654849889179|10.939470542529916|  2|\n",
      "+--------------------+-------------------+------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_with_schema.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../pictures/data_plotted.png\" alt=\"alt text\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading Data: complex structured data (Parquet)\n",
    "\n",
    "More complex data format can infer automatically schema, and data types.\n",
    "They are also optimised for fast data access and small memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+------------------+---+\n",
      "|                   x|                  y|                 z| id|\n",
      "+--------------------+-------------------+------------------+---+\n",
      "| 0.40036865101002594|  6.377802717872659|  9.12320139596368|  2|\n",
      "| 0.35619804381308917| 4.0063127514493715|2.5682278136488326|  0|\n",
      "|  1.8851627680444136|   6.11585014171703|1.7987871043042176|  0|\n",
      "| -1.7480450713588191|  7.582580700598671| 9.635550121929803|  2|\n",
      "|-0.16938263429070788|-3.2704779332785194| 3.461377027352177|  1|\n",
      "+--------------------+-------------------+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that the schema and the data types are directly inferred.\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"../data/clusters.parquet\")\n",
    "\n",
    "df_parquet.printSchema()\n",
    "\n",
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading Data: astronomy format (FITS)\n",
    "\n",
    "To read FITS, you will need to specify a custom connector such as [spark-fits](https://github.com/astrolabsoftware/spark-fits) (this is done for you):\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter-notebook pyspark --packages com.github.astrolabsoftware:spark-fits_2.12:0.9.0 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# You need to specify the HDU\n",
    "# Schema is automatically infered from the FITS header\n",
    "df_fits = spark.read.format(\"fits\").option(\"hdu\", 1).load(\"../data/clusters.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data distribution\n",
    "\n",
    "A DataFrame can be viewed as a distributed table. The table is chunked in `partitions` (i.e. one partition contains all columns, but only a subset of rows), and each Spark worker will process a subset of all partitions. When you load your data the first time, Spark will infer a default number of partitions based on several criteria, such as the type of file system (HDFS, S3, local, ...), or the initial partitioning of the dataset on disk (see next paragraph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../pictures/spark_data_distribution.png\" alt=\"alt text\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** How are distributed the different DataFrames? In other words, how many partitions has each DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# write your answer here\n",
    "print(df_csv_with_schema.rdd.getNumPartitions())\n",
    "\n",
    "print(df_parquet.rdd.getNumPartitions())\n",
    "\n",
    "print(df_fits.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note how the partitioning varies as a function of the initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitioning\n",
    "\n",
    "You might noticed Spark cut out the dataset into partitions, and for each partition Spark will run one task.\n",
    "Following the principle that moving computation is usually cheaper than moving data, Spark reads file blocks in a performant way: instead of copying file blocks to a central compute node, which can be expensive, the driver sends the computation to worker nodes close to DataNodes where the data reside.\n",
    "Normally, Spark tries to set the number of partitions automatically based on your distributed file system configuration. For example in HDFS, the size of data blocks is typically 128 MB (tunable), therefore the default number of Spark partitions when reading data will be the total number of 128 MB chunks for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How many partitions should I use?\n",
    "\n",
    "There is no unique answer to that. You will often hear: `typically you want 2-4 partitions for each CPU in your cluster`, but that implies you can accomodate infinite number of CPUs at limited partition size. In practice it will mainly depend on: \n",
    "- the total volume of data you want to distribute, \n",
    "- the number of CPU you have access to and their RAM, \n",
    "- the kind of filesystem you are using,\n",
    "- and the kind of task you want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&#9888; \n",
    "\n",
    "1. If you have too few partitions, you will not take benefit from all of the cores available in the cluster (time to solution can be longer, and you can run out of memory for intensive tasks). \n",
    "2. If you have too many partitions, there will be excessive overhead in managing many small tasks.\n",
    "\n",
    "In between, you are generally good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to repartition the data? (1/2)\n",
    "\n",
    "You can repartition the dataset using:\n",
    "\n",
    "```python\n",
    "# numPartitions is arbitrary but\n",
    "# this operation will add a shuffle step\n",
    "df.repartition(numPartitions)\n",
    "\n",
    "# Using either a number of partition or \n",
    "# column names to repartition by range\n",
    "df.repartitionByRange(numPartitions, colnames)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to repartition the data? (2/2)\n",
    "\n",
    "```python\n",
    "# Repartition and sort\n",
    "# using one or several columns\n",
    "df.orderBy(colnames)\n",
    "\n",
    "# numPartitions must be lower than the \n",
    "# current one, but no shuffle is performed\n",
    "df.coalesce(numPartitions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Frequent basic use-cases:\n",
    "- The standard: You have a lot of data stored in large files and data entries need to be process independently from each other --> keep the default.\n",
    "- The multi-files: When reading many small files (each being much smaller than the typical 128 MB data block size), you usually end up with way more partitions than if you were reading the same volume of data but with fewer files --> repartition your dataset with fewer partitions.\n",
    "- The shuffle: If your tasks involve a lot of data movement and communication between machines (data shuffle) --> it is usually a good idea to keep the number of partitions not too high.\n",
    "- The heavy filter: sometimes you filter out a lot of data based on some condition, and then you execute some action on the remaining subset. Because of the filering, you might end up with many empty partitions --> try to see if repartitioning with fewer partitions helps in processing the remaining faster.\n",
    "\n",
    "**In practice you will end up experimenting a bit with the number of partitions... But always keep in mind the main reason to repartition is to minimize data movement inside the cluster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic operations on DataFrames\n",
    "\n",
    "Let's play a bit with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# and let's load the Parquet dataset\n",
    "df = spark.read.format(\"parquet\").load(\"../data/clusters.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Select & filters\n",
    "\n",
    "There are powerful methods to select subsets of columns or to filter rows based on values. Note that column selection and row filtering are transformations (in the sense of functional programming) - nothing really happens to the data until you trigger an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: double, y: double, z: double, id: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering rows based on entry values\n",
    "df.filter(\"x > 1\")\n",
    "\n",
    "# Same as before, but different syntax\n",
    "df.filter(df[\"x\"] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also filter on column names directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering column based on their name\n",
    "df_y_only = df.select('y')\n",
    "df_x_and_y = df.select(['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can chain transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# You can chain transformations\n",
    "df_x_cluster_one = df.filter('id == 1').select('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and finally trigger an action to start the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291 entries with x > 1\n"
     ]
    }
   ],
   "source": [
    "# count() is an \"action\"\n",
    "row_with_x_more_than_one = df.filter(\"x > 1\").count()\n",
    "\n",
    "print(\"{} entries with x > 1\".format(row_with_x_more_than_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mathematical functions\n",
    "\n",
    "Many built-in functions are available in `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+---+------------------+\n",
      "|                  x|                 y|                 z| id|            radius|\n",
      "+-------------------+------------------+------------------+---+------------------+\n",
      "|0.40036865101002594| 6.377802717872659|  9.12320139596368|  2|11.138647416815433|\n",
      "|0.35619804381308917|4.0063127514493715|2.5682278136488326|  0| 4.772128771485695|\n",
      "| 1.8851627680444136|  6.11585014171703|1.7987871043042176|  0| 6.647788855294085|\n",
      "+-------------------+------------------+------------------+---+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df.select(\n",
    "    [\n",
    "        *df.columns,\n",
    "        (F.sqrt(df['x']**2 + df['y']**2 + df['z']**2)).alias('radius')\n",
    "    ]\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col)\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    acos(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n",
      "    \n",
      "    acosh(col)\n",
      "        Computes inverse hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "    \n",
      "    add_months(start, months)\n",
      "        Returns the date that is `months` months after `start`\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "    \n",
      "    aggregate(col, initialValue, merge, finish=None)\n",
      "        Applies a binary operator to an initial state and all elements in the array,\n",
      "        and reduces this to a single state. The final state is converted into the final result\n",
      "        by applying a finish function.\n",
      "        \n",
      "        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "        Python ``UserDefinedFunctions`` are not supported\n",
      "        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        initialValue : :class:`~pyspark.sql.Column` or str\n",
      "            initial value. Name of column or expression\n",
      "        merge : function\n",
      "            a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\n",
      "            of the same type as ``zero``\n",
      "        finish : function\n",
      "            an optional unary function ``(x: Column) -> Column: ...``\n",
      "            used to convert accumulated value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
      "        >>> df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
      "        +----+\n",
      "        | sum|\n",
      "        +----+\n",
      "        |42.0|\n",
      "        +----+\n",
      "        \n",
      "        >>> def merge(acc, x):\n",
      "        ...     count = acc.count + 1\n",
      "        ...     sum = acc.sum + x\n",
      "        ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
      "        >>> df.select(\n",
      "        ...     aggregate(\n",
      "        ...         \"values\",\n",
      "        ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
      "        ...         merge,\n",
      "        ...         lambda acc: acc.sum / acc.count,\n",
      "        ...     ).alias(\"mean\")\n",
      "        ... ).show()\n",
      "        +----+\n",
      "        |mean|\n",
      "        +----+\n",
      "        | 8.4|\n",
      "        +----+\n",
      "    \n",
      "    approxCountDistinct(col, rsd=None)\n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`approx_count_distinct` instead.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    approx_count_distinct(col, rsd=None)\n",
      "        Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\n",
      "        of column `col`.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        rsd : float, optional\n",
      "            maximum relative standard deviation allowed (default = 0.05).\n",
      "            For rsd < 0.01, it is more efficient to use :func:`countDistinct`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n",
      "        [Row(distinct_ages=2)]\n",
      "    \n",
      "    array(*cols)\n",
      "        Creates a new array column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that have\n",
      "            the same data type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "    \n",
      "    array_contains(col, value)\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            value or column to check for in array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "    \n",
      "    array_distinct(col)\n",
      "        Collection function: removes duplicate values from the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
      "        >>> df.select(array_distinct(df.data)).collect()\n",
      "        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n",
      "    \n",
      "    array_except(col1, col2)\n",
      "        Collection function: returns an array of the elements in col1 but not in col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_except(df.c1, df.c2)).collect()\n",
      "        [Row(array_except(c1, c2)=['b'])]\n",
      "    \n",
      "    array_intersect(col1, col2)\n",
      "        Collection function: returns an array of the elements in the intersection of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_intersect(df.c1, df.c2)).collect()\n",
      "        [Row(array_intersect(c1, c2)=['a', 'c'])]\n",
      "    \n",
      "    array_join(col, delimiter, null_replacement=None)\n",
      "        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n",
      "        `null_replacement` if set, otherwise they are ignored.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
      "        >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a')]\n",
      "        >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a,NULL')]\n",
      "    \n",
      "    array_max(col)\n",
      "        Collection function: returns the maximum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_max(df.data).alias('max')).collect()\n",
      "        [Row(max=3), Row(max=10)]\n",
      "    \n",
      "    array_min(col)\n",
      "        Collection function: returns the minimum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_min(df.data).alias('min')).collect()\n",
      "        [Row(min=1), Row(min=-1)]\n",
      "    \n",
      "    array_position(col, value)\n",
      "        Collection function: Locates the position of the first occurrence of the given value\n",
      "        in the given array. Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if the given\n",
      "        value could not be found in the array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_position(df.data, \"a\")).collect()\n",
      "        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n",
      "    \n",
      "    array_remove(col, element)\n",
      "        Collection function: Remove all elements that equal to element from the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        element :\n",
      "            element to be removed from the array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
      "        >>> df.select(array_remove(df.data, 1)).collect()\n",
      "        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n",
      "    \n",
      "    array_repeat(col, count)\n",
      "        Collection function: creates an array containing a column repeated count times.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['data'])\n",
      "        >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n",
      "        [Row(r=['ab', 'ab', 'ab'])]\n",
      "    \n",
      "    array_sort(col)\n",
      "        Collection function: sorts the input array in ascending order. The elements of the input array\n",
      "        must be orderable. Null elements will be placed at the end of the returned array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    array_union(col1, col2)\n",
      "        Collection function: returns an array of the elements in the union of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_union(df.c1, df.c2)).collect()\n",
      "        [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]\n",
      "    \n",
      "    arrays_overlap(a1, a2)\n",
      "        Collection function: returns true if the arrays contain any common non-null element; if not,\n",
      "        returns null if both the arrays are non-empty and any of them contains a null element; returns\n",
      "        false otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
      "        >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n",
      "        [Row(overlap=True), Row(overlap=False)]\n",
      "    \n",
      "    arrays_zip(*cols)\n",
      "        Collection function: Returns a merged array of structs in which the N-th struct contains all\n",
      "        N-th values of input arrays.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns of arrays to be merged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import arrays_zip\n",
      "        >>> df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], ['vals1', 'vals2'])\n",
      "        >>> df.select(arrays_zip(df.vals1, df.vals2).alias('zipped')).collect()\n",
      "        [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]\n",
      "    \n",
      "    asc(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    asc_nulls_first(col)\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values return before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    asc_nulls_last(col)\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    ascii(col)\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asin(col)\n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n",
      "    \n",
      "    asinh(col)\n",
      "        Computes inverse hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "    \n",
      "    assert_true(col, errMsg=None)\n",
      "        Returns null if the input column is true; throws an exception with the provided error message\n",
      "        otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b, df.a).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b, 'error').alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "    \n",
      "    atan(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n",
      "    \n",
      "    atan2(col1, col2)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on y-axis\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on x-axis\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the `theta` component of the point\n",
      "            (`r`, `theta`)\n",
      "            in polar coordinates that corresponds to the point\n",
      "            (`x`, `y`) in Cartesian coordinates,\n",
      "            as if computed by `java.lang.Math.atan2()`\n",
      "    \n",
      "    atanh(col)\n",
      "        Computes inverse hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "    \n",
      "    avg(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    base64(col)\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bin(col)\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(bin(df.age).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "    \n",
      "    bitwiseNOT(col)\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    broadcast(df)\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    bround(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "    \n",
      "    bucket(numBuckets, col)\n",
      "        Partition transform function: A transform for any type that partitions\n",
      "        by a hash of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     bucket(42, \"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    cbrt(col)\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    ceil(col)\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    coalesce(*cols)\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |null|null|\n",
      "        |   1|null|\n",
      "        |null|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          null|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |null|null|             0.0|\n",
      "        |   1|null|             1.0|\n",
      "        |null|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "    \n",
      "    col(col)\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    collect_list(col)\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_list('age')).collect()\n",
      "        [Row(collect_list(age)=[2, 5, 5])]\n",
      "    \n",
      "    collect_set(col)\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_set('age')).collect()\n",
      "        [Row(collect_set(age)=[5, 2])]\n",
      "    \n",
      "    column(col)\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    concat(*cols)\n",
      "        Concatenates multiple input columns together into a single column.\n",
      "        The function works with strings, binary and compatible array columns.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd123')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "        >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n",
      "        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "    \n",
      "    concat_ws(sep, *cols)\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "    \n",
      "    conv(col, fromBase, toBase)\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "    \n",
      "    corr(col1, col2)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\n",
      "        ``col1`` and ``col2``.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "    \n",
      "    cos(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosine of the angle, as if computed by `java.lang.Math.cos()`.\n",
      "    \n",
      "    cosh(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n",
      "    \n",
      "    count(col)\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    countDistinct(col, *cols)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "    \n",
      "    covar_pop(col1, col2)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    covar_samp(col1, col2)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    crc32(col)\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "    \n",
      "    create_map(*cols)\n",
      "        Creates a new map column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that are\n",
      "            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "    \n",
      "    cume_dist()\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    current_date()\n",
      "        Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "        All calls of current_date within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    current_timestamp()\n",
      "        Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\n",
      "        column. All calls of current_timestamp within the same query return the same value.\n",
      "    \n",
      "    date_add(start, days)\n",
      "        Returns the date that is `days` days after `start`\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "    \n",
      "    date_format(date, format)\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of `datetime pattern`_. can be used.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Whenever possible, use specialized functions like `year`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "    \n",
      "    date_sub(start, days)\n",
      "        Returns the date that is `days` days before `start`\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "    \n",
      "    date_trunc(format, timestamp)\n",
      "        Returns timestamp truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n",
      "            'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "    \n",
      "    datediff(end, start)\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "    \n",
      "    dayofmonth(col)\n",
      "        Extract the day of the month of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "    \n",
      "    dayofweek(col)\n",
      "        Extract the day of the week of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "        [Row(day=4)]\n",
      "    \n",
      "    dayofyear(col)\n",
      "        Extract the day of the year of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofyear('dt').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "    \n",
      "    days(col)\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into days.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     days(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    decode(col, charset)\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    degrees(col)\n",
      "        Converts an angle measured in radians to an approximately equivalent angle\n",
      "        measured in degrees.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n",
      "    \n",
      "    dense_rank()\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    desc(col)\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    desc_nulls_first(col)\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    desc_nulls_last(col)\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    element_at(col, extraction)\n",
      "        Collection function: Returns element of array at given index in extraction if col is array.\n",
      "        Returns value for the given key in extraction if col is map.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array or map\n",
      "        extraction :\n",
      "            index to check for in array or key to check for in map\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(element_at(df.data, 1)).collect()\n",
      "        [Row(element_at(data, 1)='a'), Row(element_at(data, 1)=None)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], ['data'])\n",
      "        >>> df.select(element_at(df.data, lit(\"a\"))).collect()\n",
      "        [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]\n",
      "    \n",
      "    encode(col, charset)\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    exists(col, f)\n",
      "        Returns whether a predicate holds for one or more elements in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        :return: a :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
      "        >>> df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()\n",
      "        +------------+\n",
      "        |any_negative|\n",
      "        +------------+\n",
      "        |       false|\n",
      "        |        true|\n",
      "        +------------+\n",
      "    \n",
      "    exp(col)\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "    \n",
      "    explode_outer(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Unlike explode, if the array/map is null or empty then null is produced.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+-----+\n",
      "        | id|  an_array| key|value|\n",
      "        +---+----------+----+-----+\n",
      "        |  1|[foo, bar]|   x|  1.0|\n",
      "        |  2|        []|null| null|\n",
      "        |  3|      null|null| null|\n",
      "        +---+----------+----+-----+\n",
      "        \n",
      "        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+\n",
      "        | id|     a_map| col|\n",
      "        +---+----------+----+\n",
      "        |  1|{x -> 1.0}| foo|\n",
      "        |  1|{x -> 1.0}| bar|\n",
      "        |  2|        {}|null|\n",
      "        |  3|      null|null|\n",
      "        +---+----------+----+\n",
      "    \n",
      "    expm1(col)\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expr(str)\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(expr(\"length(name)\")).collect()\n",
      "        [Row(length(name)=5), Row(length(name)=3)]\n",
      "    \n",
      "    factorial(col)\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "    \n",
      "    filter(col, f)\n",
      "        Returns an array of elements for which a predicate holds in a given array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            A function that returns the Boolean expression.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> def after_second_quarter(x):\n",
      "        ...     return month(to_date(x)) > 6\n",
      "        >>> df.select(\n",
      "        ...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
      "        ... ).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |after_second_quarter    |\n",
      "        +------------------------+\n",
      "        |[2018-09-20, 2019-07-01]|\n",
      "        +------------------------+\n",
      "    \n",
      "    first(col, ignorenulls=False)\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "    \n",
      "    flatten(col)\n",
      "        Collection function: creates a single array from an array of arrays.\n",
      "        If a structure of nested arrays is deeper than two levels,\n",
      "        only one level of nesting is removed.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
      "        >>> df.select(flatten(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]\n",
      "    \n",
      "    floor(col)\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    forall(col, f)\n",
      "        Returns whether a predicate holds for every element in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()\n",
      "        +-------+\n",
      "        |all_foo|\n",
      "        +-------+\n",
      "        |  false|\n",
      "        |  false|\n",
      "        |   true|\n",
      "        +-------+\n",
      "    \n",
      "    format_number(col, d)\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            the column name of the numeric value to be formatted\n",
      "        d : int\n",
      "            the N decimal places\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "    \n",
      "    format_string(format, *cols)\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            string that can contain embedded format tags and used as result column's value\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in formatting\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "    \n",
      "    from_csv(col, schema, options={})\n",
      "        Parses a column containing a CSV string to a row with the specified schema.\n",
      "        Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in CSV format\n",
      "        schema :class:`~pyspark.sql.Column` or str\n",
      "            a string with schema in DDL format to use when parsing the CSV column.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1,2,3\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(a=1, b=2, c=3))]\n",
      "        >>> value = data[0][0]\n",
      "        >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(_c0=1, _c1=2, _c2=3))]\n",
      "        >>> data = [(\"   abc\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> options = {'ignoreLeadingWhiteSpace': True}\n",
      "        >>> df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(s='abc'))]\n",
      "    \n",
      "    from_json(col, schema, options={})\n",
      "        Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n",
      "        as keys type, :class:`StructType` or :class:`ArrayType` with\n",
      "        the specified schema. Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        schema : :class:`DataType` or str\n",
      "            a StructType or ArrayType of StructType to use when parsing the json column.\n",
      "        \n",
      "            .. versionchanged:: 2.3\n",
      "                the DDL-formatted string is also supported for ``schema``.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the json datasource\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n",
      "        [Row(json={'a': 1})]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        >>> schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=None))]\n",
      "        >>> data = [(1, '''[1, 2, 3]''')]\n",
      "        >>> schema = ArrayType(IntegerType())\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[1, 2, 3])]\n",
      "    \n",
      "    from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "        [Row(ts='2015-04-08 00:00:00')]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    from_utc_timestamp(timestamp, tz)\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n",
      "        renders that timestamp as a timestamp in the given time zone.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n",
      "        the given timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n",
      "    \n",
      "    get_json_object(col, path)\n",
      "        Extracts json object from a json string based on json path specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        path : str\n",
      "            path to the json object to extract\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    greatest(*cols)\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "    \n",
      "    grouping(col)\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | null|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "    \n",
      "    grouping_id(*cols)\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The list of columns should match with grouping columns exactly, or empty (means all\n",
      "        the grouping columns).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+-------------+--------+\n",
      "        | name|grouping_id()|sum(age)|\n",
      "        +-----+-------------+--------+\n",
      "        | null|            1|       7|\n",
      "        |Alice|            0|       2|\n",
      "        |  Bob|            0|       5|\n",
      "        +-----+-------------+--------+\n",
      "    \n",
      "    hash(*cols)\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n",
      "        [Row(hash=-757602832)]\n",
      "    \n",
      "    hex(col)\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "    \n",
      "    hour(col)\n",
      "        Extract the hours of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(hour('ts').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "    \n",
      "    hours(col)\n",
      "        Partition transform function: A transform for timestamps\n",
      "        to partition data into hours.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(   # doctest: +SKIP\n",
      "        ...     hours(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    hypot(col1, col2)\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    initcap(col)\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "    \n",
      "    input_file_name()\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    instr(str, substr)\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    isnan(col)\n",
      "        An expression that returns true iff the column is NaN.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "    \n",
      "    isnull(col)\n",
      "        An expression that returns true iff the column is null.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "    \n",
      "    json_tuple(col, *fields)\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        fields : str\n",
      "            fields to extract\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    kurtosis(col)\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    lag(col, offset=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `default` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "    \n",
      "    last(col, ignorenulls=False)\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "    \n",
      "    last_day(date)\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    lead(col, offset=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `default` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "    \n",
      "    least(*cols)\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "    \n",
      "    length(col)\n",
      "        Computes the character length of string data or number of bytes of binary data.\n",
      "        The length of character data includes the trailing spaces. The length of binary data\n",
      "        includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=4)]\n",
      "    \n",
      "    levenshtein(left, right)\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "    \n",
      "    lit(col)\n",
      "        Creates a :class:`~pyspark.sql.Column` of literal value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n",
      "        [Row(height=5, spark_user=True)]\n",
      "    \n",
      "    locate(substr, str, pos=1)\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        substr : str\n",
      "            a string\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a Column of :class:`pyspark.sql.types.StringType`\n",
      "        pos : int, optional\n",
      "            start position (zero based)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    log(arg1, arg2=None)\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
      "        ['0.30102', '0.69897']\n",
      "        \n",
      "        >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n",
      "        ['0.69314', '1.60943']\n",
      "    \n",
      "    log10(col)\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log1p(col)\n",
      "        Computes the natural logarithm of the given value plus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log2(col)\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()\n",
      "        [Row(log2=2.0)]\n",
      "    \n",
      "    lower(col)\n",
      "        Converts a string expression to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lpad(col, len, pad)\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "    \n",
      "    ltrim(col)\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    map_concat(*cols)\n",
      "        Returns the union of all the given maps.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_concat\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n",
      "        >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |map3                    |\n",
      "        +------------------------+\n",
      "        |{1 -> a, 2 -> b, 3 -> c}|\n",
      "        +------------------------+\n",
      "    \n",
      "    map_entries(col)\n",
      "        Collection function: Returns an unordered array of all entries in the given map.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_entries\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_entries(\"data\").alias(\"entries\")).show()\n",
      "        +----------------+\n",
      "        |         entries|\n",
      "        +----------------+\n",
      "        |[{1, a}, {2, b}]|\n",
      "        +----------------+\n",
      "    \n",
      "    map_filter(col, f)\n",
      "        Returns a map whose key-value pairs satisfy a predicate.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n",
      "        >>> df.select(map_filter(\n",
      "        ...     \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------+\n",
      "        |data_filtered             |\n",
      "        +--------------------------+\n",
      "        |{baz -> 32.0, foo -> 42.0}|\n",
      "        +--------------------------+\n",
      "    \n",
      "    map_from_arrays(col1, col2)\n",
      "        Creates a new map from two arrays.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of keys. All elements should not be null\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
      "        >>> df.select(map_from_arrays(df.k, df.v).alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |{2 -> a, 5 -> b}|\n",
      "        +----------------+\n",
      "    \n",
      "    map_from_entries(col)\n",
      "        Collection function: Returns a map created from the given array of entries.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_from_entries\n",
      "        >>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
      "        >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |{1 -> a, 2 -> b}|\n",
      "        +----------------+\n",
      "    \n",
      "    map_keys(col)\n",
      "        Collection function: Returns an unordered array containing the keys of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_keys\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n",
      "        +------+\n",
      "        |  keys|\n",
      "        +------+\n",
      "        |[1, 2]|\n",
      "        +------+\n",
      "    \n",
      "    map_values(col)\n",
      "        Collection function: Returns an unordered array containing the values of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_values\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n",
      "        +------+\n",
      "        |values|\n",
      "        +------+\n",
      "        |[a, b]|\n",
      "        +------+\n",
      "    \n",
      "    map_zip_with(col1, col2, f)\n",
      "        Merge two given maps, key-wise into a single map using a function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a ternary function ``(k: Column, v1: Column, v2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\n",
      "        ...     (\"id\", \"base\", \"ratio\")\n",
      "        ... )\n",
      "        >>> df.select(map_zip_with(\n",
      "        ...     \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\n",
      "        ... ).show(truncate=False)\n",
      "        +---------------------------+\n",
      "        |updated_data               |\n",
      "        +---------------------------+\n",
      "        |{SALES -> 16.8, IT -> 48.0}|\n",
      "        +---------------------------+\n",
      "    \n",
      "    max(col)\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    md5(col)\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "    \n",
      "    mean(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    min(col)\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    minute(col)\n",
      "        Extract the minutes of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(minute('ts').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "    \n",
      "    monotonically_increasing_id()\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its result depends on partition IDs.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
      "        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n",
      "    \n",
      "    month(col)\n",
      "        Extract the month of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(month('dt').alias('month')).collect()\n",
      "        [Row(month=4)]\n",
      "    \n",
      "    months(col)\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into months.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(\n",
      "        ...     months(\"ts\")\n",
      "        ... ).createOrReplace()  # doctest: +SKIP\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    months_between(date1, date2, roundOff=True)\n",
      "        Returns number of months between dates date1 and date2.\n",
      "        If date1 is later than date2, then the result is positive.\n",
      "        If date1 and date2 are on the same day of month, or both are the last day of month,\n",
      "        returns an integer (time of day will be ignored).\n",
      "        The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "        [Row(months=3.94959677)]\n",
      "        >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "        [Row(months=3.9495967741935485)]\n",
      "    \n",
      "    nanvl(col1, col2)\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "    \n",
      "    next_day(date, dayOfWeek)\n",
      "        Returns the first date which is later than the value of the date column.\n",
      "        \n",
      "        Day of the week parameter is case insensitive, and accepts:\n",
      "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "    \n",
      "    nth_value(col, offset, ignoreNulls=False)\n",
      "        Window function: returns the value that is the `offset`\\th row of the window frame\n",
      "        (counting from 1), and `null` if the size of window frame is less than `offset` rows.\n",
      "        \n",
      "        It will return the `offset`\\th non-null value it sees when `ignoreNulls` is set to\n",
      "        true. If all values are null, then null is returned.\n",
      "        \n",
      "        This is equivalent to the nth_value function in SQL.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional\n",
      "            number of row to use as the value\n",
      "        ignoreNulls : bool, optional\n",
      "            indicates the Nth value should skip null in the\n",
      "            determination of which row to use\n",
      "    \n",
      "    ntile(n)\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            an integer\n",
      "    \n",
      "    overlay(src, replace, pos, len=-1)\n",
      "        Overlay the specified portion of `src` with `replace`,\n",
      "        starting from byte position `pos` of `src` and proceeding for `len` bytes.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).show()\n",
      "        +----------+\n",
      "        | overlayed|\n",
      "        +----------+\n",
      "        |SPARK_CORE|\n",
      "        +----------+\n",
      "    \n",
      "    percent_rank()\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    percentile_approx(col, percentage, accuracy=10000)\n",
      "        Returns the approximate `percentile` of the numeric column `col` which is the smallest value\n",
      "        in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\n",
      "        of `col` values is less than the value or equal to that value.\n",
      "        The value of percentage must be between 0.0 and 1.0.\n",
      "        \n",
      "        The accuracy parameter (default: 10000)\n",
      "        is a positive numeric literal which controls approximation accuracy at the cost of memory.\n",
      "        Higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error\n",
      "        of the approximation.\n",
      "        \n",
      "        When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\n",
      "        In this case, returns the approximate percentile array of column col\n",
      "        at the given percentage array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> key = (col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- quantiles: array (nullable = true)\n",
      "         |    |-- element: double (containsNull = false)\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     percentile_approx(\"value\", 0.5, lit(1000000)).alias(\"median\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- key: long (nullable = true)\n",
      "         |-- median: double (nullable = true)\n",
      "    \n",
      "    posexplode(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(posexplode(eDF.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> eDF.select(posexplode(eDF.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    posexplode_outer(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+----+-----+\n",
      "        | id|  an_array| pos| key|value|\n",
      "        +---+----------+----+----+-----+\n",
      "        |  1|[foo, bar]|   0|   x|  1.0|\n",
      "        |  2|        []|null|null| null|\n",
      "        |  3|      null|null|null| null|\n",
      "        +---+----------+----+----+-----+\n",
      "        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+----+\n",
      "        | id|     a_map| pos| col|\n",
      "        +---+----------+----+----+\n",
      "        |  1|{x -> 1.0}|   0| foo|\n",
      "        |  1|{x -> 1.0}|   1| bar|\n",
      "        |  2|        {}|null|null|\n",
      "        |  3|      null|null|null|\n",
      "        +---+----------+----+----+\n",
      "    \n",
      "    pow(col1, col2)\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    quarter(col)\n",
      "        Extract the quarter of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(quarter('dt').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "    \n",
      "    radians(col)\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle\n",
      "        measured in radians.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in degrees\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in radians, as if computed by `java.lang.Math.toRadians()`\n",
      "    \n",
      "    raise_error(errMsg)\n",
      "        Throws an exception with the provided error message.\n",
      "        \n",
      "        .. versionadded:: 3.1\n",
      "    \n",
      "    rand(seed=None)\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        uniformly distributed in [0.0, 1.0).\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n",
      "        [Row(age=2, name='Alice', rand=2.4052597283576684),\n",
      "         Row(age=5, name='Bob', rand=2.3913904055683974)]\n",
      "    \n",
      "    randn(seed=None)\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumn('randn', randn(seed=42)).collect()\n",
      "        [Row(age=2, name='Alice', randn=1.1027054481455365),\n",
      "        Row(age=5, name='Bob', randn=0.7400395449950132)]\n",
      "    \n",
      "    rank()\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    regexp_extract(str, pattern, idx)\n",
      "        Extract a specific group matched by a Java regex, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "    \n",
      "    regexp_replace(str, pattern, replacement)\n",
      "        Replace all substrings of the specified string value that match regexp with rep.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "    \n",
      "    repeat(col, n)\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "    \n",
      "    reverse(col)\n",
      "        Collection function: returns a reversed string or an array with reverse order of elements.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('s')).collect()\n",
      "        [Row(s='LQS krapS')]\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('r')).collect()\n",
      "        [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    rint(col)\n",
      "        Returns the double value that is closest in value to the argument and\n",
      "        is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    round(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    row_number()\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    rpad(col, len, pad)\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "    \n",
      "    rtrim(col)\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    schema_of_csv(csv, options={})\n",
      "        Parses a CSV string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        csv : :class:`~pyspark.sql.Column` or str\n",
      "            a CSV string or a foldable string column containing a CSV string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<`_c0`: INT, `_c1`: STRING>')]\n",
      "        >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<`_c0`: INT, `_c1`: STRING>')]\n",
      "    \n",
      "    schema_of_json(json, options={})\n",
      "        Parses a JSON string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        json : :class:`~pyspark.sql.Column` or str\n",
      "            a JSON string or a foldable string column containing a JSON string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the JSON datasource\n",
      "        \n",
      "            .. versionchanged:: 3.0\n",
      "               It accepts `options` parameter to control schema inferring.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<`a`: BIGINT>')]\n",
      "        >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n",
      "        >>> df.select(schema.alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<`a`: BIGINT>')]\n",
      "    \n",
      "    second(col)\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(second('ts').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "    \n",
      "    sequence(start, stop, step=None)\n",
      "        Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n",
      "        If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n",
      "        otherwise -1.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
      "        >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n",
      "        [Row(r=[-2, -1, 0, 1, 2])]\n",
      "        >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
      "        >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n",
      "        [Row(r=[4, 2, 0, -2, -4])]\n",
      "    \n",
      "    sha1(col)\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "    \n",
      "    sha2(col, numBits)\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> digests = df.select(sha2(df.name, 256).alias('s')).collect()\n",
      "        >>> digests[0]\n",
      "        Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')\n",
      "        >>> digests[1]\n",
      "        Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')\n",
      "    \n",
      "    shiftLeft(col, numBits)\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "    \n",
      "    shiftRight(col, numBits)\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "    \n",
      "    shiftRightUnsigned(col, numBits)\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "    \n",
      "    shuffle(col)\n",
      "        Collection function: Generates a random permutation of the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
      "        >>> df.select(shuffle(df.data).alias('s')).collect()  # doctest: +SKIP\n",
      "        [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\n",
      "    \n",
      "    signum(col)\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sin(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sine of the angle, as if computed by `java.lang.Math.sin()`\n",
      "    \n",
      "    sinh(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic sine of the given value,\n",
      "            as if computed by `java.lang.Math.sinh()`\n",
      "    \n",
      "    size(col)\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "    \n",
      "    skewness(col)\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    slice(x, start, length)\n",
      "        Collection function: returns an array containing  all the elements in `x` from index `start`\n",
      "        (array indices start at 1, or from the end if `start` is negative) with the specified `length`.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the array to be sliced\n",
      "        start : :class:`~pyspark.sql.Column` or int\n",
      "            the starting index\n",
      "        length : :class:`~pyspark.sql.Column` or int\n",
      "            the length of the slice\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
      "        >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n",
      "        [Row(sliced=[2, 3]), Row(sliced=[5])]\n",
      "    \n",
      "    sort_array(col, asc=True)\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements. Null elements will be placed at the beginning\n",
      "        of the returned array in ascending order or at the end of the returned array in descending\n",
      "        order.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        asc : bool, optional\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    soundex(col)\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "    \n",
      "    spark_partition_id()\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is non deterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "    \n",
      "    split(str, pattern, limit=-1)\n",
      "        Splits str around matches of the given pattern.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a string expression to split\n",
      "        pattern : str\n",
      "            a string representing a regular expression. The regex string should be\n",
      "            a Java regular expression.\n",
      "        limit : int, optional\n",
      "            an integer which controls the number of times `pattern` is applied.\n",
      "        \n",
      "            * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
      "                             resulting array's last entry will contain all input beyond the last\n",
      "                             matched pattern.\n",
      "            * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
      "                              array can be of any size.\n",
      "        \n",
      "            .. versionchanged:: 3.0\n",
      "               `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
      "        [Row(s=['one', 'twoBthreeC'])]\n",
      "        >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
      "        [Row(s=['one', 'two', 'three', ''])]\n",
      "    \n",
      "    sqrt(col)\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    stddev(col)\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_pop(col)\n",
      "        Aggregate function: returns population standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_samp(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    struct(*cols)\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, set, str or :class:`~pyspark.sql.Column`\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to contain in the output struct.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "    \n",
      "    substring(str, pos, len)\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "    \n",
      "    substring_index(str, delim, count)\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "    \n",
      "    sum(col)\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    sumDistinct(col)\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    tan(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            tangent of the given value, as if computed by `java.lang.Math.tan()`\n",
      "    \n",
      "    tanh(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic tangent of the given value\n",
      "            as if computed by `java.lang.Math.tanh()`\n",
      "    \n",
      "    timestamp_seconds(col)\n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import timestamp_seconds\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2008-12-25 07:30:00|\n",
      "        +-------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    toDegrees(col)\n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`degrees` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toRadians(col)\n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`radians` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    to_csv(col, options={})\n",
      "        Converts a column containing a :class:`StructType` into a CSV string.\n",
      "        Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct.\n",
      "        options: dict, optional\n",
      "            options to control converting. accepts the same options as the CSV datasource.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n",
      "        [Row(csv='2,Alice')]\n",
      "    \n",
      "    to_date(col, format=None)\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    to_json(col, options={})\n",
      "        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n",
      "        into a JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct, an array or a map.\n",
      "        options : dict, optional\n",
      "            options to control converting. accepts the same options as the JSON datasource.\n",
      "            Additionally the function supports the `pretty` option which enables\n",
      "            pretty JSON generation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(age=2, name='Alice'), Row(age=3, name='Bob')])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, {\"name\": \"Alice\"})]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, [\"Alice\", \"Bob\"])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[\"Alice\",\"Bob\"]')]\n",
      "    \n",
      "    to_timestamp(col, format=None)\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    to_utc_timestamp(timestamp, tz)\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n",
      "        timezone, and renders that timestamp as a timestamp in UTC.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n",
      "        timezone to UTC timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            upported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4.0\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        >>> df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n",
      "    \n",
      "    transform(col, f)\n",
      "        Returns an array of elements after applying a transformation to each element in the input array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a function that is applied to each element of the input array.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
      "        >>> df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()\n",
      "        +------------+\n",
      "        |     doubled|\n",
      "        +------------+\n",
      "        |[2, 4, 6, 8]|\n",
      "        +------------+\n",
      "        \n",
      "        >>> def alternate(x, i):\n",
      "        ...     return when(i % 2 == 0, x).otherwise(-x)\n",
      "        >>> df.select(transform(\"values\", alternate).alias(\"alternated\")).show()\n",
      "        +--------------+\n",
      "        |    alternated|\n",
      "        +--------------+\n",
      "        |[1, -2, 3, -4]|\n",
      "        +--------------+\n",
      "    \n",
      "    transform_keys(col, f)\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new keys for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n",
      "        >>> df.select(transform_keys(\n",
      "        ...     \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n",
      "        ... ).show(truncate=False)\n",
      "        +-------------------------+\n",
      "        |data_upper               |\n",
      "        +-------------------------+\n",
      "        |{BAR -> 2.0, FOO -> -2.0}|\n",
      "        +-------------------------+\n",
      "    \n",
      "    transform_values(col, f)\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new values for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n",
      "        >>> df.select(transform_values(\n",
      "        ...     \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n",
      "        ... ).alias(\"new_data\")).show(truncate=False)\n",
      "        +---------------------------------------+\n",
      "        |new_data                               |\n",
      "        +---------------------------------------+\n",
      "        |{OPS -> 34.0, IT -> 20.0, SALES -> 2.0}|\n",
      "        +---------------------------------------+\n",
      "    \n",
      "    translate(srcCol, matching, replace)\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        The translate will happen when any character in the string matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "    \n",
      "    trim(col)\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trunc(date, format)\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "    \n",
      "    udf(f=None, returnType=StringType)\n",
      "        Creates a user defined function (UDF).\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            python function if used as a standalone function\n",
      "        returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The user-defined functions are considered deterministic by default. Due to\n",
      "        optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "        more times than it is present in the query. If your function is not deterministic, call\n",
      "        `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> import random\n",
      "        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "        \n",
      "        The user-defined functions do not support conditional expressions or short circuiting\n",
      "        in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "        can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        The user-defined functions do not take keyword arguments on the calling side.\n",
      "    \n",
      "    unbase64(col)\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unhex(col)\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "    \n",
      "    unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, return null if fail.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "        [Row(unix_time=1428476400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    upper(col)\n",
      "        Converts a string expression to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    var_pop(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    var_samp(col)\n",
      "        Aggregate function: returns the unbiased sample variance of\n",
      "        the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    variance(col)\n",
      "        Aggregate function: alias for var_samp\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    weekofyear(col)\n",
      "        Extract the week number of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "    \n",
      "    when(condition, value)\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "        conditions.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`~pyspark.sql.Column`\n",
      "            a boolean :class:`~pyspark.sql.Column` expression.\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=4)]\n",
      "        \n",
      "        >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=None)]\n",
      "    \n",
      "    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "    \n",
      "    xxhash64(*cols)\n",
      "        Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\n",
      "        and returns the result as a long column.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(xxhash64('a').alias('hash')).collect()\n",
      "        [Row(hash=4105715581806190027)]\n",
      "    \n",
      "    year(col)\n",
      "        Extract the year of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(year('dt').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "    \n",
      "    years(col)\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into years.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     years(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    zip_with(left, right, f)\n",
      "        Merge two given arrays, element-wise, into a single array using a function.\n",
      "        If one array is shorter, nulls are appended at the end to match the length of the longer\n",
      "        array, before applying the function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a binary function ``(x1: Column, x2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)\n",
      "        +---------------------------+\n",
      "        |powers                     |\n",
      "        +---------------------------+\n",
      "        |[1.0, 9.0, 625.0, 262144.0]|\n",
      "        +---------------------------+\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()\n",
      "        +-----------------+\n",
      "        |            xs_ys|\n",
      "        +-----------------+\n",
      "        |[foo_1, bar_2, 3]|\n",
      "        +-----------------+\n",
      "\n",
      "FILE\n",
      "    /usr/local/spark/python/pyspark/sql/functions.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistics\n",
    "\n",
    "You can easily access basics statistics of your DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-------------------+------------------+\n",
      "|summary|                  x|                 y|                  z|                id|\n",
      "+-------+-------------------+------------------+-------------------+------------------+\n",
      "|  count|               4000|              4000|               4000|              4000|\n",
      "|   mean|0.22461143161189473|3.5005327477749755|  4.746261685611469|           0.99975|\n",
      "| stddev| 1.4333802737826389| 3.970358011802802|  3.385895822783182|0.8166496596868618|\n",
      "|    min| -4.320974828599122|-5.207575440768161|-1.4595005976690572|                 0|\n",
      "|    max|  4.077800662643146|10.854512466048538| 12.602016902866591|                 2|\n",
      "+-------+-------------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The describe method returns a DataFrame\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aggregation\n",
    "\n",
    "Apache Spark has built-in method to perform aggregation. Be careful though - this implies shuffle (i.e. communication between machines and data transfer), and can be a performance killer (naively, this scales quadratically in the number of partitions)!\n",
    "\n",
    "<img src=\"../pictures/shuffle.png\" alt=\"alt text\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** group by `id`, and count the number of elements per `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1| 1333|\n",
      "|  2| 1333|\n",
      "|  0| 1334|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('id').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** Using `groupBy` and `agg`, compute the barycentre for each ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+------------------+\n",
      "| id|             avg(x)|             avg(y)|            avg(z)|\n",
      "+---+-------------------+-------------------+------------------+\n",
      "|  1| 0.9084311322436581|-1.5335608883132903| 2.926201255363396|\n",
      "|  2|-1.2364938227997027|   7.78371632274562| 9.292937669035528|\n",
      "|  0| 1.0013143125628066|  4.250879907797302|2.0216900721305437|\n",
      "+---+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('id').agg({'x': 'mean', 'y': 'mean', 'z': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logical plan & physical plan\n",
    "\n",
    "As quickly highlighted above, Spark commands are either transformations (filter, select, ...) or actions (show, take, ...). You can chain actions, and in the end you trigger the computation with an action. Before running any action, Spark will build a the graphs of the commands, called Direct Acyclic Graphs. One is the logical plan (what you wrote), the other is the physical plan (what will run). And... it will do some magic for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** Look at the two commands and outputs. Do you notice the magic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['id], [unresolvedalias('id, None), count(1) AS count#247L]\n",
      "+- Filter (id#237 >= 1)\n",
      "   +- Relation[x#234,y#235,z#236,id#237] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, count: bigint\n",
      "Aggregate [id#237], [id#237, count(1) AS count#247L]\n",
      "+- Filter (id#237 >= 1)\n",
      "   +- Relation[x#234,y#235,z#236,id#237] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [id#237], [id#237, count(1) AS count#247L]\n",
      "+- Project [id#237]\n",
      "   +- Filter (isnotnull(id#237) AND (id#237 >= 1))\n",
      "      +- Relation[x#234,y#235,z#236,id#237] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[id#237], functions=[count(1)], output=[id#237, count#247L])\n",
      "+- Exchange hashpartitioning(id#237, 200), ENSURE_REQUIREMENTS, [id=#204]\n",
      "   +- *(1) HashAggregate(keys=[id#237], functions=[partial_count(1)], output=[id#237, count#251L])\n",
      "      +- *(1) Filter (isnotnull(id#237) AND (id#237 >= 1))\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [id#237] Batched: true, DataFilters: [isnotnull(id#237), (id#237 >= 1)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThanOrEqual(id,1)], ReadSchema: struct<id:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('id >= 1').groupBy(\"id\").count().explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('id >= 1)\n",
      "+- Aggregate [id#116], [id#116, count(1) AS count#578L]\n",
      "   +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, count: bigint\n",
      "Filter (id#116 >= 1)\n",
      "+- Aggregate [id#116], [id#116, count(1) AS count#578L]\n",
      "   +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [id#116], [id#116, count(1) AS count#578L]\n",
      "+- Project [id#116]\n",
      "   +- Filter (isnotnull(id#116) AND (id#116 >= 1))\n",
      "      +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[id#116], functions=[count(1)], output=[id#116, count#578L])\n",
      "+- Exchange hashpartitioning(id#116, 200), ENSURE_REQUIREMENTS, [id=#599]\n",
      "   +- *(1) HashAggregate(keys=[id#116], functions=[partial_count(1)], output=[id#116, count#584L])\n",
      "      +- *(1) Filter (isnotnull(id#116) AND (id#116 >= 1))\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [id#116] Batched: true, DataFilters: [isnotnull(id#116), (id#116 >= 1)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThanOrEqual(id,1)], ReadSchema: struct<id:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"id\").count().filter('id >= 1').explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The DAG is a powerful tool to debug and optimize performance. Let's look at a common mistake: repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217 ms Â± 47.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "67.5 ms Â± 4.98 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Let's fake a DataFrame with many partitions\n",
    "# and let's cache the result on executors\n",
    "df_tmp = df.repartition(32).cache()\n",
    "df_tmp.count()\n",
    "\n",
    "# Now, let's measure the time to repartition\n",
    "# using `repartition` vs `coalesce`\n",
    "%timeit df_tmp.repartition(2).count()\n",
    "%timeit df_tmp.coalesce(2).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are different by several factors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Exchange RoundRobinPartitioning(32), REPARTITION_WITH_NUM, [id=#7290]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [x#90,y#91,z#92,id#93] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<x:double,y:double,z:double,id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(2).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Coalesce 32\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [x#90,y#91,z#92,id#93] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<x:double,y:double,z:double,id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.coalesce(2).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to the driver\n",
    "\n",
    "Once you have filtered, selected, and processed your data on the executors, you would like to return to the driver to finish your analysis (plotting, or perhaps just quick data mining). There are several ways of doing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_sub = df.filter('y < 0.5').filter('x > 1').select(['x', 'id'])\n",
    "\n",
    "# Collect only 10 elements\n",
    "some_rows = df_sub.take(10)\n",
    "\n",
    "# Collect the data as a list of Rows\n",
    "rows = df_sub.collect()\n",
    "\n",
    "# Collect data as pandas DataFrame\n",
    "pdf = df_sub.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.015219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.656493</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.347220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.146386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.043088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>2.306735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1.233722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.869804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1.145487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>1.037731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x  id\n",
       "0    2.015219   1\n",
       "1    1.656493   1\n",
       "2    1.347220   1\n",
       "3    1.146386   1\n",
       "4    2.043088   1\n",
       "..        ...  ..\n",
       "597  2.306735   1\n",
       "598  1.233722   1\n",
       "599  1.869804   1\n",
       "600  1.145487   1\n",
       "601  1.037731   1\n",
       "\n",
       "[602 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "WARNING: Use these methods only after reducing the data volume! These methods take the data left after processing on ALL the executors (it can be terabytes), and send it to the RAM of the driver. In case you do not know how much you've reduced, use `df_sub.count()` before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "<img src=\"../pictures/logo-Escape_0.png\" alt=\"alt text\" width=\"400\" align=\"right\"/>\n",
    "\n",
    "This event is organized in the framework and with the support of the European Science Cluster of Astronomy & Particle physics ESFRI research infrastructures (ESCAPE), funded by the European Union's Horizon 2020 - Grant N. 824064."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
