{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ESCAPE Summer School 2021: Big data for big science #1\n",
    "\n",
    "<img src=\"../pictures/spark_escape_logo.png\" alt=\"alt text\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Last run: 2021-06-03\n",
    "* Reading time: ~40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Setup for Colab_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context\n",
    "\n",
    "Welcome to the series of notebooks on Apache Spark! The main goal of this series is to get familiar with Apache Spark, and in particular its Python API called PySpark in a scientific context. In this first notebook, we will introduce few Apache Spark functionalities of interest (and by no means complete!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Apache Spark\n",
    "\n",
    "<img src=\"../pictures/spark_timeline.png\" alt=\"alt text\"/>\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/) is a cluster computing framework, that is a set of tools to perform computation on a network of many machines. Spark started in 2009 as a research project, and it had a huge success so far in the industry. It is based on the so-called MapReduce cluster computing paradigm, popularized by the Hadoop framework using implicit data parallelism and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../pictures/scala_logo.png\" alt=\"alt text\" width=\"200\"/>\n",
    "\n",
    "The core of Spark is written in Scala which is a general-purpose programming language that has been started in 2004 by Martin Odersky (EPFL). The language is inter-operable with Java and Java-like languages, and Scala executables run on the Java Virtual Machine (JVM). While Scala contains many ideas from functional programming, it is multi-paradigm, including functional programming, imperative programming, object-oriented programming and concurrent computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark provides many functionalities exposed through Scala/Python/Java/R API (Scala being the native one). As far as this lecture is concerned, we will use the Python API (called PySpark). But feel free to put your hands on Scala, it's worth it. For those interested, you can have a look at this [tutorial](https://gitlab.in2p3.fr/MaitresNageurs/QuatreNages/Scala) on Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning objectives for session 1\n",
    "\n",
    "- Loading and distributing data with Spark SQL (Apache Spark Data Sources API)\n",
    "- Exploring DataFrame & partitioning\n",
    "- Manipulating Spark SQL built-in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributed computing\n",
    "\n",
    "<img src=\"../pictures/spark_cluster.png\" alt=\"alt text\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Spark cluster\n",
    "\n",
    "Spark runs on clusters of machines, and typically in a cloud environment. In a simple deploy mode, a cluster is made of a driver node, connected via ssh to several worker nodes. Each worker is typically linked to a distributed file system (HDFS, Ceph, Amazon S3, ...). Two execution modes: _client_ or _cluster_ mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Resource Scheduling\n",
    "\n",
    "Resource scheduling and optimisation can be a difficult operation if done manually (the standalone mode of Spark). Alternatively, Spark can run on clusters managed by cluster managers such as YARN, Mesos, or even Kubernetes. See https://spark.apache.org/docs/latest/#launching-on-a-cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running applications step-by-step\n",
    "\n",
    "1. `Driver` contacts the `cluster manager` and requests for resources to launch the `Executors` inside the `Workers`.\n",
    "2. The `cluster manager` launches the `Executors` on behalf of the `Driver`.\n",
    "3. Once the `Executors` are launched, they establish a direct connection with the `Driver`.\n",
    "4. The `Driver` determines the total number of `Tasks` by checking the `Lineage`.\n",
    "5. The `Driver` creates the `Logical` and `Physical Plan`.\n",
    "6. Once the `Physical Plan` is generated, Spark allocates the `Tasks` to the `Executors`.\n",
    "7. `Task` runs on `Executor` and each `Task` upon completion returns the result to the `Driver`.\n",
    "8. Finally, when all `Tasks` are completed, the program exits, and Spark releases all the resources from the `Cluster Manager`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to run Spark applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "# Launching a script\n",
    "spark-submit [OPTIONS] mysparkscript.py [OPTIONS]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "# Launching a python shell (ipython)\n",
    "PYSPARK_DRIVER_PYTHON=ipython pyspark [OPTIONS]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "# Launching a jupyter-notebook\n",
    "PYSPARK_DRIVER_PYTHON=jupyter-notebook pyspark [OPTIONS]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "There are 4 main libraries in Apache Spark:\n",
    "1. **SQL & DataFrames**\n",
    "2. Streaming & Structured Streaming\n",
    "3. MLlib (Machine Learning)\n",
    "4. GraphX (graph)\n",
    "\n",
    "For this lecture, we will focus on the SQL & DataFrames library. This library lets you query structured data inside Apache Spark programs, using either SQL or a familiar DataFrame API (à la Pandas). Note that the other libraries are also using the functionnalities of the SQL & DataFrames one (unified interface)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Spark Data Sources\n",
    "\n",
    "### A tour of data formats\n",
    "\n",
    "- Many data formats used in the context of Big Data: CSV (1978), XML (1996), JSON (2001), Thrift (2007), Protobuf (2008), Avro & SequenceFile (2009), Parquet (2013), ORC (2016), and the list goes on... \n",
    "- Some are _naively_ structured versus more complex and highly optimised for big data treatment (e.g. Parquet). \n",
    "\n",
    "Unfortunately those are often not the data formats typically chosen by the scientific community, e.g.: FITS (1981), HDF5 (1988) format, or ROOT (1995). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Connecting to Data Source\n",
    "\n",
    "The data source API in Apache Spark belongs to the [Spark SQL module](https://spark.apache.org/sql/). Note that Spark Core has some simple built-in ways to read data from disk (binary or text), but Spark SQL is more complete and give you access to DataFrames directly. If you want to connect a specific data source with Apache Spark:\n",
    "\n",
    "- [indirect] Access and distribute your files as binary streams (Spark does it natively), and decode the data on-the-fly within executors using third-party libraries --> _Easy to do, but poor performance._\n",
    "- [native] Use a built-in or custom connector to access, distribute and decode the data natively --> _More challenging to write, but excellent performance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Connectors for scientific data formats?\n",
    "\n",
    "By default, Spark cannot read natively most scientific formats like FITS, HDF5, or ROOT. There were several attempts though to build connectors: \n",
    "\n",
    "[1] Z. Zhang et al, Kira: Processing Astronomy Imagery Using Big Data Technology, DOI 10.1109/TBDATA.2016.2599926.    \n",
    "[2] Peloton et al, FITS Data Source for Apache Spark, Computing and Software for Big Science (1804.07501). https://github.com/astrolabsoftware/spark-fits   \n",
    "[3] Liu et al, H5spark: bridging the I/O gap between Spark and scientific data formats on HPC systems, Cray user group (2016). https://github.com/valiantljk/h5spark  \n",
    "[4] Viktor Khristenko, & Jim Pivarski. (2017, October 20). diana-hep/spark-root: Release 0.1.14 (Version v0.1.14). Zenodo. http://doi.org/10.5281/zenodo.1034230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Most of scientific data formats, were not designed for serialisation (distribution of data over machines) originally and they often use compression to reduce the size on disk. Needless to say that default Spark cannot read those natively.\n",
    "\n",
    "First attempts to connect those data formats (see e.g. [1] for FITS) with Spark were using the indirect method above. By reading files as binary streams, the indirect method has the advantage of having access to all FITS functionalities implemented in the underlying user library. This can be an advantage when working with the Python API for example which already contains many great scientific libraries. However this indirect method assumes each Spark mapper will receive and handle one entire file (since the filenames are parallelized and entire file data must be reconstructed from binary once the file has been opened by a Spark mapper). Therefore each single file must fit within the memory of a Spark mapper, hence the indirect method cannot distribute a dataset made of large FITS files (e.g. in [1] they have a 65 GB dataset made of 11,150 files). In addition by assuming each Spark mapper will receive and handle one entire file, the indirect method will have a poor load balancing if the dataset is made of files with not all the same size.\n",
    "\n",
    "Fortunately Apache Spark low-level layers are sufficiently well written to allow extending the framework and write native connectors for any kind of data sources. Recently connectors for FITS, HDF5 and ROOT were made available [2, 3, 4] to the community. With such connectors, there is a guarantee of having a good load balancing regardless the structure of the dataset and the size of the input files is no more a problem (a 1 TB dataset made of thousand 1 GB files or one single 1 TB file will be viewed as almost the same by a native Spark connector). Note however that the Data Source API is in Java/Scala and if there is no library to play with your data source in those languages you must implement it (what has been done in [2]) or interface with another language.\n",
    "\n",
    "Note that the low-level layers dealing with the data sources have been recently updated. Apache Spark 2.3 introduced the Data Source API version 2. While the version 1 is still available and usable for a long time, we expect that all Spark connectors will comply with this v2 in the future.\n",
    "\n",
    "[1] Z. Zhang and K. Barbary and F. A. Nothaft and E. R. Sparks and O. Zahn and M. J. Franklin and D. A. Patterson and S. Perlmutter, Kira: Processing Astronomy Imagery Using Big Data Technology, DOI 10.1109/TBDATA.2016.2599926.  \n",
    "[2] Peloton, Julien and Arnault, Christian and Plaszczynski, Stéphane, FITS Data Source for Apache Spark, Computing and Software for Big Science (1804.07501). https://github.com/astrolabsoftware/spark-fits   \n",
    "[3] Liu, Jialin and Racah, Evan and Koziol, Quincey and Canon, Richard Shane, H5spark: bridging the I/O gap between Spark and scientific data formats on HPC systems, Cray user group (2016). https://github.com/valiantljk/h5spark  \n",
    "[4] Viktor Khristenko, & Jim Pivarski. (2017, October 20). diana-hep/spark-root: Release 0.1.14 (Version v0.1.14). Zenodo. http://doi.org/10.5281/zenodo.1034230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark hands-on: entry point\n",
    "\n",
    "By default, this notebook comes with several instantiated objects to interact with the Spark cluster you just created by launching this notebook. The most important is the Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fcb1db3a6d8b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f123cbe93d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark Session will allow you to read data, and manage the configuration of the cluster. Note you can also create it (or get the current one running) via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataFrameReader\n",
    "\n",
    "The interface to read data from disk is always the same for any kind of built-in and officially supported data format:\n",
    "\n",
    "```python\n",
    "df = spark.read\\\n",
    "    .format(format: str)\\\n",
    "    .option(key: str, value: Any)\\\n",
    "    # ...\n",
    "    .option(key: str, value: Any)\\\n",
    "    .load(path: str)\n",
    "```\n",
    " \n",
    "Note that for most of the data sources, you can use wrappers such as:\n",
    "\n",
    "```python\n",
    "spark.read.csv(path, key1=value1, key2=value2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Format**: The format can be \"csv\", \"json\", \"parquet\", etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Options**: The number of options depends on the underlying data source. Each has its own set of options. \n",
    "In most of the case, no options are needed, but you might want to explore the different possibilities at some point. Surprisingly it is not easy to find documentation and the best remains to read the source code documentation. In pyspark you can easily access it via the wrappers:\n",
    "\n",
    "```python\n",
    "# DataFrameReader object\n",
    "df_reader = spark.read\n",
    "\n",
    "# Doc on reading CSV\n",
    "df_reader.csv?\n",
    "# doc printed\n",
    "\n",
    "# Doc on reading Parquet\n",
    "df_reader.parquet?\n",
    "# doc printed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Path**: The way to specify path is threefold: either a single file (`path/to/folder/myfile.source`), or an entire folder (`path/to/folder`), or a glob pattern (`path/to/folder/*pattern*.source`). Note that you also need to specify the type of file system you are using. Example:\n",
    "\n",
    "``` python\n",
    "# Connect to hdfs\n",
    "path = 'hdfs:///path/to/data'\n",
    "\n",
    "# Connect to S3\n",
    "path = 's3:///path/to/data'\n",
    "\n",
    "# Connect to local file system\n",
    "path = 'files:///path/to/data'\n",
    "```\n",
    "\n",
    "If nothing is specified (`'/path/to/data'`), it will adapt to your `--master` (e.g. if you launch spark in local mode, you will connect to the local file system by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using a custom connector\n",
    "\n",
    "You can also connect to custom connector not included in the default Spark distribution. To do so, you will need to specify the dependencies when submitting your job or invoking your shell. If your connector is available through [Maven Central Repository](https://search.maven.org/), you can easily specify it via:\n",
    "\n",
    "```bash\n",
    "# Direct download from central repository\n",
    "spark-submit --packages groupId:artifactId:version ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that this is the same syntax when launching the `pyspark` shell.\n",
    "For example, if you want to read FITS files using the [spark-fits](https://github.com/astrolabsoftware/spark-fits) connector you would add the following:\n",
    "\n",
    "```bash\n",
    "# Direct download from central repository\n",
    "spark-submit --packages com.github.astrolabsoftware:spark-fits_2.12:0.9.0 ...\n",
    "```\n",
    "\n",
    "You can find the spark-fits entry in the Maven Central [here](https://search.maven.org/artifact/com.github.astrolabsoftware/spark-fits_2.12/0.9.0/jar) for reference.\n",
    "Alternatively you can download the source code for a particular connector, compile it and include the `jars`:\n",
    "\n",
    "```bash\n",
    "# Specify manually the dependency\n",
    "spark-submit --jars /path/to/lib/spark-fits.jars ...\n",
    "```\n",
    "\n",
    "Note that when you launch `pyspark`, already a numbers of `jars` are included by default (the ones for Spark for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading and distributing data\n",
    "\n",
    "You will find test data in the folder `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some historical consideration\n",
    "\n",
    "The main data object in Spark is the `DataFrame`. It was not always the case. Spark started with the concept of `RDD`. RDDs are distributed memory abstractions, fault-tolerant and immutable. They are designed for a general reuse (e.g. performing interactive data mining).\n",
    "\n",
    "A `DataFrame` is a distributed collection of records, like a `RDD`, organized into named columns and including the benefits of Spark SQL’s execution engine. This is a new interface added in Spark version 1.6. Of course, the `RDD` interface is still accessible, and sometimes very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading Data: simply structured data (text)\n",
    "\n",
    "You can load CSV data into a DataFrame by simply using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load simple CSV file\n",
    "df_csv_simple = spark.read.format(\"csv\")\\\n",
    "    .load(\"../data/clusters.csv\")\n",
    "df_csv_simple.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice by default the CSV connector interprets all entries as String, and give dummy names to columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** How would you infer the data type and use the first row as column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No schema information\n",
    "df_csv_simple = spark.read.format(\"csv\")\\\n",
    "    .load(\"../data/clusters.csv\")\n",
    "\n",
    "# Add schema information\n",
    "df_csv_with_schema = spark.read.format(\"csv\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load(\"../data/clusters.csv\")\n",
    "\n",
    "df_csv_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading Data: complex structured data (Parquet)\n",
    "\n",
    "More complex data format can infer automatically schema, and data types.\n",
    "They are also optimised for fast data access and small memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+------------------+---+\n",
      "|                   x|                  y|                 z| id|\n",
      "+--------------------+-------------------+------------------+---+\n",
      "| 0.40036865101002594|  6.377802717872659|  9.12320139596368|  2|\n",
      "| 0.35619804381308917| 4.0063127514493715|2.5682278136488326|  0|\n",
      "|  1.8851627680444136|   6.11585014171703|1.7987871043042176|  0|\n",
      "| -1.7480450713588191|  7.582580700598671| 9.635550121929803|  2|\n",
      "|-0.16938263429070788|-3.2704779332785194| 3.461377027352177|  1|\n",
      "+--------------------+-------------------+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that the schema and the data types are directly inferred.\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"../data/clusters.parquet\")\n",
    "\n",
    "df_parquet.printSchema()\n",
    "\n",
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading Data: astronomy format (FITS)\n",
    "\n",
    "To read FITS, you will need to specify a custom connector such as [spark-fits](https://github.com/astrolabsoftware/spark-fits) (this is done for you):\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter-notebook pyspark --packages com.github.astrolabsoftware:spark-fits_2.12:0.9.0 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# You need to specify the HDU\n",
    "# Schema is automatically infered from the FITS header\n",
    "df_fits = spark.read.format(\"fits\").option(\"hdu\", 1).load(\"../data/clusters.fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data distribution\n",
    "\n",
    "A DataFrame can be viewed as a distributed table. The table is chunked in `partitions` (i.e. one partition contains all columns, but only a subset of rows), and each Spark worker will process a subset of all partitions. When you load your data the first time, Spark will infer a default number of partitions based on several criteria, such as the type of file system (HDFS, S3, local, ...), or the initial partitioning of the dataset on disk (see next paragraph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../pictures/spark_data_distribution.png\" alt=\"alt text\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** How are distributed the different DataFrames? In other words, how many partitions has each DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# write your answer here\n",
    "print(df_csv_with_schema.rdd.getNumPartitions())\n",
    "\n",
    "print(df_parquet.rdd.getNumPartitions())\n",
    "\n",
    "print(df_fits.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note how the partitioning varies as a function of the initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitioning\n",
    "\n",
    "You might noticed Spark cut out the dataset into partitions, and for each partition Spark will run one task.\n",
    "Following the principle that moving computation is usually cheaper than moving data, Spark reads file blocks in a performant way: instead of copying file blocks to a central compute node, which can be expensive, the driver sends the computation to worker nodes close to DataNodes where the data reside.\n",
    "Normally, Spark tries to set the number of partitions automatically based on your distributed file system configuration. For example in HDFS, the size of data blocks is typically 128 MB (tunable), therefore the default number of Spark partitions when reading data will be the total number of 128 MB chunks for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How many partitions should I use?\n",
    "\n",
    "There is no unique answer to that. You will often hear: `typically you want 2-4 partitions for each CPU in your cluster`, but that implies you can accomodate infinite number of CPUs at limited partition size. In practice it will mainly depend on: \n",
    "- the total volume of data you want to distribute, \n",
    "- the number of CPU you have access to and their RAM, \n",
    "- the kind of filesystem you are using,\n",
    "- and the kind of task you want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&#9888; \n",
    "\n",
    "1. If you have too few partitions, you will not take benefit from all of the cores available in the cluster (time to solution can be longer, and you can run out of memory for intensive tasks). \n",
    "2. If you have too many partitions, there will be excessive overhead in managing many small tasks.\n",
    "\n",
    "In between, you are generally good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to repartition the data? (1/2)\n",
    "\n",
    "You can repartition the dataset using:\n",
    "\n",
    "```python\n",
    "# numPartitions is arbitrary but\n",
    "# this operation will add a shuffle step\n",
    "df.repartition(numPartitions)\n",
    "\n",
    "# Using either a number of partition or \n",
    "# column names to repartition by range\n",
    "df.repartitionByRange(numPartitions, colnames)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to repartition the data? (2/2)\n",
    "\n",
    "```python\n",
    "# Repartition and sort\n",
    "# using one or several columns\n",
    "df.orderBy(colnames)\n",
    "\n",
    "# numPartitions must be lower than the \n",
    "# current one, but no shuffle is performed\n",
    "df.coalesce(numPartitions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Frequent basic use-cases:\n",
    "- The standard: You have a lot of data stored in large files and data entries need to be process independently from each other --> keep the default.\n",
    "- The multi-files: When reading many small files (each being much smaller than the typical 128 MB data block size), you usually end up with way more partitions than if you were reading the same volume of data but with fewer files --> repartition your dataset with fewer partitions.\n",
    "- The shuffle: If your tasks involve a lot of data movement and communication between machines (data shuffle) --> it is usually a good idea to keep the number of partitions not too high.\n",
    "- The heavy filter: sometimes you filter out a lot of data based on some condition, and then you execute some action on the remaining subset. Because of the filering, you might end up with many empty partitions --> try to see if repartitioning with fewer partitions helps in processing the remaining faster.\n",
    "\n",
    "**In practice you will end up experimenting a bit with the number of partitions... But always keep in mind the main reason to repartition is to minimize data movement inside the cluster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic operations on DataFrames\n",
    "\n",
    "Let's play a bit with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# and let's load the Parquet dataset\n",
    "df = spark.read.format(\"parquet\").load(\"../data/clusters.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Select & filters\n",
    "\n",
    "There are powerful methods to select subsets of columns or to filter rows based on values. Note that column selection and row filtering are transformations (in the sense of functional programming) - nothing really happens to the data until you trigger an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: double, y: double, z: double, id: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering rows based on entry values\n",
    "df.filter(\"x > 1\")\n",
    "\n",
    "# Same as before, but different syntax\n",
    "df.filter(df[\"x\"] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also filter on column names directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering column based on their name\n",
    "df_y_only = df.select('y')\n",
    "df_x_and_y = df.select(['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can chain transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# You can chain transformations\n",
    "df_x_cluster_one = df.filter('id == 1').select('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and finally trigger an action to start the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291 entries with x > 1\n"
     ]
    }
   ],
   "source": [
    "# count() is an \"action\"\n",
    "row_with_x_more_than_one = df.filter(\"x > 1\").count()\n",
    "\n",
    "print(\"{} entries with x > 1\".format(row_with_x_more_than_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Map and mapPartitions\n",
    "\n",
    "You can also apply transformation on DataFrame values. The most simple transformation would use the `map` method which preserves the cardinality of the DataFrame. `mapPartitions` is similar, although the cardinality is not preserved. Note, you need to use the \"legacy\" RDD interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+---+\n",
      "|                   x|                 y|                 z| id|\n",
      "+--------------------+------------------+------------------+---+\n",
      "|  0.8007373020200519|12.755605435745318| 18.24640279192736|  4|\n",
      "|  0.7123960876261783| 8.012625502898743| 5.136455627297665|  0|\n",
      "|  3.7703255360888273| 12.23170028343406|3.5975742086084352|  0|\n",
      "| -3.4960901427176383|15.165161401197341|19.271100243859607|  4|\n",
      "|-0.33876526858141576|-6.540955866557039| 6.922754054704354|  2|\n",
      "+--------------------+------------------+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def multiply_by_two(row: float) -> float:\n",
    "    \"\"\" Multiply each element by two\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row: Any numeric type (int, float, double, ...)\n",
    "        Entry of our DataFrame\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    new_row: Any numeric type (int, float, double, ...)\n",
    "    \"\"\"\n",
    "    new_row = [2*i for i in row]\n",
    "    return new_row\n",
    "\n",
    "# map is a RDD method (not available for DataFrame in pyspark)\n",
    "df.rdd.map(multiply_by_two).toDF(df.columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows per Spark partitions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[333, 334, 334, 334, 334, 334, 333, 333, 333, 332, 333, 333]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example for mapPartitions: count the number of rows per partition\n",
    "def yield_num_rows(part):\n",
    "    \"\"\" Yield the number of rows in the partition\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    part : Iterator\n",
    "        Iterator containing partition data\n",
    "        \n",
    "    Yield\n",
    "    ----------\n",
    "    length: integer\n",
    "        number of rows inside the partition\n",
    "    \"\"\"\n",
    "    partition_data = [*part]\n",
    "    yield len(partition_data)\n",
    "\n",
    "# Let's repartition our DataFrame in 12 partitions\n",
    "df_repart = df.repartition(12)\n",
    "\n",
    "# mapPartitions is a RDD method(not available for DataFrame in pyspark)\n",
    "print(\"Number of rows per Spark partitions:\")\n",
    "df_repart.rdd.mapPartitions(yield_num_rows).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice the super good load balancing! Be careful though in using `collect`, as data flows from the executors to the (poor and lonely and undersized) driver. Always reducing the data first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise**: You noticed that the DataFrame has 4 columns: 3D position (`x`, `y`, `z`) and an ID number. Using a mapPartition, compute the barycentre for each ID (hint: repartition or re-order according to the `id` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster coordinates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.00131431, 4.25087991, 2.02169007]),\n",
       " array([ 0.90843113, -1.53356089,  2.92620126]),\n",
       " array([-1.23649382,  7.78371632,  9.29293767]),\n",
       " [None, None, None]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def yield_barycentre(part):\n",
    "    \"\"\" Yield the number of rows in the partition\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    part : Iterator\n",
    "        Iterator containing partition data\n",
    "        \n",
    "    Yield\n",
    "    ----------\n",
    "    length: integer\n",
    "        number of rows inside the partition\n",
    "    \"\"\"\n",
    "    try:\n",
    "        partition_data = [*part]\n",
    "        x, y, z, _ = np.transpose(partition_data)\n",
    "        yield np.mean([x, y, z], axis=1)\n",
    "    except ValueError as e:\n",
    "        # Empty partition\n",
    "        yield [None, None, None]\n",
    "\n",
    "# Let's repartition our DataFrame according to \"id\"\n",
    "df_repart = df.orderBy(\"id\")\n",
    "\n",
    "# mapPartitions is a RDD method(not available for DataFrame in pyspark)\n",
    "print(\"Cluster coordinates:\")\n",
    "df_repart.rdd.mapPartitions(yield_barycentre).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistics\n",
    "\n",
    "You can easily access basics statistics of your DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-------------------+------------------+\n",
      "|summary|                  x|                 y|                  z|                id|\n",
      "+-------+-------------------+------------------+-------------------+------------------+\n",
      "|  count|               4000|              4000|               4000|              4000|\n",
      "|   mean|0.22461143161189473|3.5005327477749755|  4.746261685611469|           0.99975|\n",
      "| stddev| 1.4333802737826389| 3.970358011802802|  3.385895822783182|0.8166496596868618|\n",
      "|    min| -4.320974828599122|-5.207575440768161|-1.4595005976690572|                 0|\n",
      "|    max|  4.077800662643146|10.854512466048538| 12.602016902866591|                 2|\n",
      "+-------+-------------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The describe method returns a DataFrame\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aggregation\n",
    "\n",
    "Apache Spark has built-in method to perform aggregation. Be careful though - this implies shuffle (i.e. communication between machines and data transfer), and can be a performance killer!\n",
    "\n",
    "**Exercise:** group by `id`, and count the number of elements per `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1| 1333|\n",
      "|  2| 1333|\n",
      "|  0| 1334|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('id').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** Using `groupBy` and `agg`, compute the barycentre for each ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.77 s ± 550 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.groupBy('id').agg({'x': 'mean', 'y': 'mean', 'z': 'mean'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note on timing (think about it):\n",
    "\n",
    "```python\n",
    "\n",
    "%timeit df.orderBy(\"id\").rdd.mapPartitions(yield_barycentre).count()\n",
    "168 ms ± 44.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "%timeit df.groupBy('id').agg({'x': 'mean', 'y': 'mean', 'z': 'mean'}).count()\n",
    "1.53 s ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logical plan & physical plan\n",
    "\n",
    "As quickly highlighted above, Spark commands are either transformations (filter, select, ...) or actions (show, take, ...). You can chain actions, and in the end you trigger the computation with an action. Before running any action, Spark will build a the graphs of the commands, called Direct Acyclic Graphs. One is the logical plan (what you wrote), the other is the physical plan (what will run). And... it will do some magic for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** Look at the two commands and outputs. Do you notice the magic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('id >= 1)\n",
      "+- Aggregate [id#116], [id#116, count(1) AS count#578L]\n",
      "   +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, count: bigint\n",
      "Filter (id#116 >= 1)\n",
      "+- Aggregate [id#116], [id#116, count(1) AS count#578L]\n",
      "   +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [id#116], [id#116, count(1) AS count#578L]\n",
      "+- Project [id#116]\n",
      "   +- Filter (isnotnull(id#116) AND (id#116 >= 1))\n",
      "      +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[id#116], functions=[count(1)], output=[id#116, count#578L])\n",
      "+- Exchange hashpartitioning(id#116, 200), ENSURE_REQUIREMENTS, [id=#599]\n",
      "   +- *(1) HashAggregate(keys=[id#116], functions=[partial_count(1)], output=[id#116, count#584L])\n",
      "      +- *(1) Filter (isnotnull(id#116) AND (id#116 >= 1))\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [id#116] Batched: true, DataFilters: [isnotnull(id#116), (id#116 >= 1)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThanOrEqual(id,1)], ReadSchema: struct<id:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"id\").count().filter('id >= 1').explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['id], [unresolvedalias('id, None), count(1) AS count#590L]\n",
      "+- Filter (id#116 >= 1)\n",
      "   +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, count: bigint\n",
      "Aggregate [id#116], [id#116, count(1) AS count#590L]\n",
      "+- Filter (id#116 >= 1)\n",
      "   +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [id#116], [id#116, count(1) AS count#590L]\n",
      "+- Project [id#116]\n",
      "   +- Filter (isnotnull(id#116) AND (id#116 >= 1))\n",
      "      +- Relation[x#113,y#114,z#115,id#116] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[id#116], functions=[count(1)], output=[id#116, count#590L])\n",
      "+- Exchange hashpartitioning(id#116, 200), ENSURE_REQUIREMENTS, [id=#631]\n",
      "   +- *(1) HashAggregate(keys=[id#116], functions=[partial_count(1)], output=[id#116, count#594L])\n",
      "      +- *(1) Filter (isnotnull(id#116) AND (id#116 >= 1))\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [id#116] Batched: true, DataFilters: [isnotnull(id#116), (id#116 >= 1)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThanOrEqual(id,1)], ReadSchema: struct<id:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('id >= 1').groupBy(\"id\").count().explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Coming back to our timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[id#116], functions=[avg(x#113), avg(y#114), avg(z#115)])\n",
      "+- Exchange hashpartitioning(id#116, 200), ENSURE_REQUIREMENTS, [id=#655]\n",
      "   +- *(1) HashAggregate(keys=[id#116], functions=[partial_avg(x#113), partial_avg(y#114), partial_avg(z#115)])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [x#113,y#114,z#115,id#116] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<x:double,y:double,z:double,id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('id').agg({'x': 'mean', 'y': 'mean', 'z': 'mean'}).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [id#116 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(id#116 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#674]\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [x#113,y#114,z#115,id#116] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/clusters.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<x:double,y:double,z:double,id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"id\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Under the hood, `groupBy` and `orderBy` are using different methods to repartition the data: `hashpartitioning` and `rangepartitioning` respectively."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
